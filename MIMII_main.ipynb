{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:31:31.616054Z",
     "start_time": "2020-05-11T13:31:31.613337Z"
    }
   },
   "source": [
    "# About\n",
    "\n",
    "![prjpic](doc/media_main/story.png)\n",
    "\n",
    "This is the main demo of this repo, it is about a concept study on the MIMII dataset to detect anomalies of machines or machine parts like fans, slider, pump and valves by means of classic machine learning and deep learning methods.\n",
    "\n",
    "In runs through the essentials to demonstrate the steps\n",
    "* feature extraction\n",
    "* indvitual model training \n",
    "* ensamble building and varfication\n",
    "* summery and scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T14:09:28.091750Z",
     "start_time": "2020-05-20T14:09:28.039751Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load feature_extractor_mother\n",
      "load feature_extractor_mel_spectra\n",
      "load feature_extractor_psd\n",
      "load feature_extractor_ICA2\n",
      "load feature_extractore_pre_nnFilterDenoise\n",
      "load extractor_diagram_mother\n",
      "load Simple_FIR_HP\n",
      "load TimeSliceAppendActivation\n",
      "load load_data\n",
      "Load split_data\n",
      "Load anomaly_detection_models\n",
      "Load pseudo_supervised_models\n",
      "Load tensorflow models\n",
      "Load detection_pipe\n",
      "load # extractor diagram V1 essential\n",
      "load extractor_batch\n"
     ]
    }
   ],
   "source": [
    "#===============================================\n",
    "# Basic Imports\n",
    "BASE_FOLDER = './'\n",
    "TARGET_FOLDER_FE = r'\\dataset\\extdia_v1_essential' # output folder for ffeat. extraction\n",
    "# import the repo-local utility py files\n",
    "%run -i utility\\feature_extractor\\JupyterLoad_feature_extractor.py\n",
    "%run -i utility\\modeling\\JupyterLoad_modeling.py\n",
    "\n",
    "# feature extraction diagram\n",
    "%run -i feature_extraction_diagrams\\extdia_v1_essential\n",
    "%run -i utility\\extractor_batch.py\n",
    "\n",
    "# helper\n",
    "from tqdm.auto import tqdm\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "# sklearn \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "\n",
    "feat_ext_folder = os.path.abspath(BASE_FOLDER + TARGET_FOLDER_FE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General config\n",
    "\n",
    "To customize the following notebook execution, set the specifications here:\n",
    "\n",
    "The execution time on a local desktop PC for all four IDs of one machine and one SNR is circa 1 hour. We recommend only executing a set of 4 variations at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T14:09:29.076199Z",
     "start_time": "2020-05-20T14:09:29.072197Z"
    }
   },
   "outputs": [],
   "source": [
    "#===============================================\n",
    "# Possible variations\n",
    "# \n",
    "# SNRs = ['6dB', '0dB', 'min6dB']\n",
    "# machines = ['pump', 'valve', 'fan', 'slider']\n",
    "# IDs = ['00', '02', '04', '06']\n",
    "\n",
    "SNRs = ['6dB']\n",
    "machines = ['pump']\n",
    "IDs = ['00']\n",
    "\n",
    "# note: increase n_jobs to max. CPUs you have use all hyperthreading cores (there is no auto detect just now)\n",
    "n_jobs = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:39:10.674321Z",
     "start_time": "2020-05-11T13:39:10.671328Z"
    }
   },
   "source": [
    "## Utility wrapper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T14:09:30.216467Z",
     "start_time": "2020-05-20T14:09:30.208468Z"
    }
   },
   "outputs": [],
   "source": [
    "def feat_ext_process_set(FileFindDict, main_channel=0, sporadic=False, augment=False,FileCountLimit=None, n_jobs=4):\n",
    "    \n",
    "    if sporadic:\n",
    "        dt = 1 # 1 means time slicing \n",
    "    else:\n",
    "        dt = 0\n",
    "        \n",
    "    if augment:\n",
    "        ag = 0 # augment only normal operation\n",
    "    else:\n",
    "        ag = -2 # not existing class = no augment\n",
    "        \n",
    "    extractor_batch(base_folder= BASE_FOLDER, \n",
    "                    target_folder=TARGET_FOLDER_FE, \n",
    "                    extdia = extdia_v1_essential, \n",
    "                    FileFindDict = FileFindDict,\n",
    "                    n_jobs = n_jobs,\n",
    "                    target_class_map = {'abnormal':1, 'normal': 0},\n",
    "                    FileCountLimit = FileCountLimit,\n",
    "                    datset_folder_from_base = 'dataset',\n",
    "                    fHP=120,\n",
    "                    DeviceType=dt,\n",
    "                    main_channel = main_channel,\n",
    "                    augment=ag)\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T14:09:30.760233Z",
     "start_time": "2020-05-20T14:09:30.753233Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_data_file(SNR, machine, ID):\n",
    "    '''\n",
    "    function to find existing feature data files\n",
    "    '''\n",
    "    path = glob.glob(BASE_FOLDER \n",
    "              + '/dataset/extdia_v1_essential/{}{}{}_EDiaV1HP'.format(machine, SNR, ID) \n",
    "              + \"*pandaDisc*.pkl\", recursive=True)\n",
    "    \n",
    "    if len(path) == 0:\n",
    "        return None\n",
    "    elif len(path) == 1:\n",
    "        return path[0]\n",
    "    else:\n",
    "        raise Exception('More than one file found:', path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feat Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:40:58.788472Z",
     "start_time": "2020-05-11T13:40:58.786459Z"
    }
   },
   "source": [
    "## Confirm feature extraction diagram\n",
    "\n",
    "![exdia](doc/media_feature_extraction/exdia_v1_essential.png)\n",
    "in order to modify the diagram go the class definition:  /feature_extraction_diagrams/extdia_v1_essential.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note to the main_channel\n",
    "The main channel is picking one microphone out of the 8, \n",
    "this can be seen as if the demo version is strictly in working mono\n",
    "Or a DOA could be used to find the main direction see : feature_extraction_diagrams/A21_DirectionOfArrival_DOA/pyroomacustic_DOA.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T11:41:07.790440Z",
     "start_time": "2020-05-20T11:41:07.783440Z"
    },
    "scrolled": true
   },
   "source": [
    "if not find_data_file('6dB', 'pump', '02'):\n",
    "    ExampleFileFilter = {'SNR': ['6dB'],'machine': ['pump'],'ID': ['02']}\n",
    "    # create some \n",
    "    feat_ext_process_set(ExampleFileFilter,\n",
    "                        main_channel=2,\n",
    "                        sporadic = False,\n",
    "                        augment = True,\n",
    "                        FileCountLimit= 4,\n",
    "                        n_jobs=n_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spot Check the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T11:41:11.353733Z",
     "start_time": "2020-05-20T11:41:08.738734Z"
    }
   },
   "source": [
    "# This code reloads pkl files that have been stored\n",
    "# in the step above - notice only created files can be loaded\n",
    "# then a plot is made form n and n+1 output ports\n",
    "# this cell is ment as a spot check before running the batch that might,\n",
    "# take much more time !\n",
    "\n",
    "d_MEL_den = pickle.load( open( feat_ext_folder + r'\\pump6dB02_EDiaV1HPaug0_outpMEL_den.pkl', \"rb\" ))\n",
    "d_MEL_raw = pickle.load( open( feat_ext_folder + r'\\pump6dB02_EDiaV1HPaug0_outpMEL_raw.pkl', \"rb\" ))\n",
    "d_PSD_raw = pickle.load( open( feat_ext_folder + r'\\pump6dB02_EDiaV1HPaug0_outpPSD_raw.pkl', \"rb\" ))\n",
    "n1=2\n",
    "n2=7\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.subplot(321)\n",
    "feature_extractor_from_dict(d_MEL_raw[n1],BASE_FOLDER).plot(False)\n",
    "plt.subplot(322)\n",
    "feature_extractor_from_dict(d_MEL_raw[n2],BASE_FOLDER).plot(False)\n",
    "plt.subplot(323)\n",
    "feature_extractor_from_dict(d_MEL_den[n1],BASE_FOLDER).plot(False)\n",
    "plt.subplot(324)\n",
    "feature_extractor_from_dict(d_MEL_den[n2],BASE_FOLDER).plot(False)\n",
    "plt.subplot(325)\n",
    "feature_extractor_from_dict(d_PSD_raw[n1],BASE_FOLDER).plot(True)\n",
    "plt.subplot(326)\n",
    "feature_extractor_from_dict(d_PSD_raw[n2],BASE_FOLDER).plot(True)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch creation of feature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T14:09:36.793083Z",
     "start_time": "2020-05-20T14:09:36.785084Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the batch of feature data\n",
    "# note: there is still a deepcopy issue you may expirence memory leak : https://github.com/BA-HanseML/NF_Prj_MIMII_Dataset/issues/58\n",
    "for SNR in SNRs:\n",
    "    for machine in machines:\n",
    "        for ID in IDs:\n",
    "            # check if files already exist\n",
    "            if not find_data_file(SNR, machine, ID):\n",
    "                BatchFileFilter = {'SNR': SNR,'machine': machine,'ID': ID}\n",
    "                feat_ext_process_set(BatchFileFilter,\n",
    "                                    main_channel=2,\n",
    "                                    sporadic = False,\n",
    "                                    augment = True,\n",
    "                                    n_jobs=n_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T08:19:01.289568Z",
     "start_time": "2020-05-20T08:19:01.282569Z"
    }
   },
   "source": [
    "# PSD_raw_flat_6dB_pump_ID00_SVM\n",
    "\n",
    "def find_pipe(feature, SNR, machine, ID, model):\n",
    "    '''\n",
    "    function to find existing model pipe files\n",
    "    '''\n",
    "    path = glob.glob(BASE_FOLDER \n",
    "              + '/pipes/{}_*_{}_{}_ID{}_{}_*.pkl'.format(feature, SNR, machine, ID, model), recursive=True)\n",
    "    \n",
    "    if len(path) == 0:\n",
    "        return None\n",
    "    elif len(path) == 1:\n",
    "        return path[0]\n",
    "    else:\n",
    "        raise Exception('More than one file found:', path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T14:10:16.062716Z",
     "start_time": "2020-05-20T14:10:16.037716Z"
    }
   },
   "outputs": [],
   "source": [
    "class uni_Ensemble(object):\n",
    "    def __init__(self, SNR, machine, ID):\n",
    "        self.SNR = SNR\n",
    "        self.machine = machine\n",
    "        self.ID = ID\n",
    "        \n",
    "        self.weights = [0.9, 0.8]\n",
    "        \n",
    "        self.tasks = [{\n",
    "                    'path_descr': find_data_file(SNR, machine, ID),\n",
    "                    'feat':feature[1], \n",
    "                    'feat_col':feature[0], \n",
    "                    'SNR':SNR, \n",
    "                    'machine':machine,\n",
    "                    'ID':ID,\n",
    "                    'BASE_FOLDER':BASE_FOLDER\n",
    "        } for feature in [\n",
    "                         ('PSD_raw', {'function':'flat'}), # Isolation Forest Welch method\n",
    "                         ('PSD_raw', {'function':'flat'})]] # SVM augmented Welch method\n",
    "        \n",
    "        self.pipes = [\n",
    "            Pipe(preprocessing_steps=[(StandardScaler, {})], \n",
    "                 modeling_step=(uni_IsolationForest, {'n_estimators':200, 'max_features':1}),\n",
    "                 pseudo_sup=False), # Isolation Forest Welch method\n",
    "            \n",
    "            Pipe(preprocessing_steps=[(StandardScaler, {})], \n",
    "                 modeling_step=(uni_svm, {'C': 0.1, 'degree':3,'kernel':'rbf'}),\n",
    "                 pseudo_sup=True), # SVM augmented Welch method\n",
    "        ]\n",
    "        \n",
    "    def fit(self):\n",
    "        for pipe, task in zip(self.pipes, self.tasks):\n",
    "            \n",
    "            # set up the task\n",
    "            pipe.task = task\n",
    "            \n",
    "            # split data into train and testset\n",
    "            pipe.split_data()\n",
    "            \n",
    "            # get the data\n",
    "            print('...loading data')\n",
    "            data_train, data_test = pipe.get_data()\n",
    "            print('data loading completed\\n\\n...preprocessing data')\n",
    "\n",
    "            # preprocessing\n",
    "            data_train, data_test = pipe.preprocess(data_train, data_test)\n",
    "            print('data preprocessing finished\\n\\n...fitting the model')\n",
    "\n",
    "            # fitting the model\n",
    "            pipe.fit_model(data_train)\n",
    "            print('model fitted successfully\\n\\n...fitting the prediction scaler')\n",
    "\n",
    "            # fitting the prediction scaler\n",
    "            pipe.fit_score_scaler(data_train)\n",
    "            print('prediction scaler fitted successfully\\n\\n...evaluating model')\n",
    "\n",
    "            # evaluating over ground truth\n",
    "            pipe.evaluate(data_test)\n",
    "            print('evaluation successfull, roc_auc:', pipe.roc_auc)\n",
    "            \n",
    "            \n",
    "    def predict(self, data):\n",
    "        predictions = np.array([])\n",
    "        for pipe, weight in zip(self.pipes, self.weights):\n",
    "            data = pipe.preprocess_post(data)\n",
    "            np.append(predictions, np.expand_dims(pipe.predict_score(data)*weight, axis=1), axis=1)\n",
    "            \n",
    "        prediction = np.sum(predictions, axis=0)\n",
    "        return prediction\n",
    "    \n",
    "    def evaluate():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T14:10:16.446916Z",
     "start_time": "2020-05-20T14:10:16.441916Z"
    }
   },
   "outputs": [],
   "source": [
    "ensemble = uni_Ensemble('6dB', 'pump', '00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T14:13:23.963304Z",
     "start_time": "2020-05-20T14:10:17.153314Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".//dataset/extdia_v1_essential\\pump6dB00_EDiaV1HPaug0_pandaDisc.pkl --> Done\n",
      "...loading data\n",
      "data loading completed\n",
      "\n",
      "...preprocessing data\n",
      "data preprocessing finished\n",
      "\n",
      "...fitting the model\n",
      "Epoch 1/5\n",
      "1049/1049 [==============================] - 10s 9ms/step - loss: 0.9371\n",
      "Epoch 2/5\n",
      "1049/1049 [==============================] - 7s 7ms/step - loss: 0.8825\n",
      "Epoch 3/5\n",
      "1049/1049 [==============================] - 7s 7ms/step - loss: 0.8590\n",
      "Epoch 4/5\n",
      "1049/1049 [==============================] - 7s 6ms/step - loss: 0.8404\n",
      "Epoch 5/5\n",
      "1049/1049 [==============================] - 7s 6ms/step - loss: 0.8275\n",
      "model fitted successfully\n",
      "\n",
      "...fitting the prediction scaler\n",
      "prediction scaler fitted successfully\n",
      "\n",
      "...evaluating model\n",
      "evaluation successfull, roc_auc: 0.9798497430233669\n",
      ".//dataset/extdia_v1_essential\\pump6dB00_EDiaV1HPaug0_pandaDisc.pkl --> Done\n",
      "...loading data\n",
      "data loading completed\n",
      "\n",
      "...preprocessing data\n",
      "data preprocessing finished\n",
      "\n",
      "...fitting the model\n",
      "model fitted successfully\n",
      "\n",
      "...fitting the prediction scaler\n",
      "prediction scaler fitted successfully\n",
      "\n",
      "...evaluating model\n",
      "evaluation successfull, roc_auc: 0.9789813696188214\n",
      ".//dataset/extdia_v1_essential\\pump6dB00_EDiaV1HPaug0_pandaDisc.pkl --> Done\n",
      "...loading data\n",
      "data loading completed\n",
      "\n",
      "...preprocessing data\n",
      "data preprocessing finished\n",
      "\n",
      "...fitting the model\n",
      "model fitted successfully\n",
      "\n",
      "...fitting the prediction scaler\n",
      "prediction scaler fitted successfully\n",
      "\n",
      "...evaluating model\n",
      "evaluation successfull, roc_auc: 0.9600958482077363\n",
      ".//dataset/extdia_v1_essential\\pump6dB00_EDiaV1HPaug0_pandaDisc.pkl --> Done\n",
      "...loading data\n",
      "data loading completed\n",
      "\n",
      "...preprocessing data\n",
      "data preprocessing finished\n",
      "\n",
      "...fitting the model\n",
      "0.9835573177723562\n",
      "model fitted successfully\n",
      "\n",
      "...fitting the prediction scaler\n",
      "prediction scaler fitted successfully\n",
      "\n",
      "...evaluating model\n",
      "evaluation successfull, roc_auc: 0.9968213604577241\n"
     ]
    }
   ],
   "source": [
    "ensemble.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T14:15:06.618320Z",
     "start_time": "2020-05-20T14:15:06.415545Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_2 to have shape (64,) but got array with shape (513,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mD:\\Capstone\\NF_Prj_MIMII_Dataset\\utility\\extractor_batch.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mensemble\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Capstone\\NF_Prj_MIMII_Dataset\\utility\\extractor_batch.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Capstone\\NF_Prj_MIMII_Dataset\\utility\\extractor_batch.py\u001b[0m in \u001b[0;36mpredict_score\u001b[1;34m(self, data)\u001b[0m\n",
      "\u001b[1;32mD:\\Capstone\\NF_Prj_MIMII_Dataset\\utility\\extractor_batch.py\u001b[0m in \u001b[0;36mpredict_score\u001b[1;34m(self, data)\u001b[0m\n",
      "\u001b[1;32mD:\\Capstone\\NF_Prj_MIMII_Dataset\\utility\\extractor_batch.py\u001b[0m in \u001b[0;36mpredict_raw\u001b[1;34m(self, data)\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\mimii-tf2-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 909\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    910\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\mimii-tf2-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    460\u001b[0m     return self._model_iteration(\n\u001b[0;32m    461\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m         steps=steps, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\mimii-tf2-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[1;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    394\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m           \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 396\u001b[1;33m           distribution_strategy=strategy)\n\u001b[0m\u001b[0;32m    397\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\mimii-tf2-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m         steps=steps)\n\u001b[0m\u001b[0;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[0;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\mimii-tf2-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2470\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2471\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2472\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m     \u001b[1;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\mimii-tf2-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    572\u001b[0m                              \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 574\u001b[1;33m                              str(data_shape))\n\u001b[0m\u001b[0;32m    575\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_2 to have shape (64,) but got array with shape (513,)"
     ]
    }
   ],
   "source": [
    "ensemble.predict(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up supervised training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T14:09:20.610299Z",
     "start_time": "2020-05-20T14:09:20.439Z"
    }
   },
   "outputs": [],
   "source": [
    "#===============================================\n",
    "# Adding up all the tasks\n",
    "tasks = []\n",
    "pipes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T08:52:23.045467Z",
     "start_time": "2020-05-20T08:52:23.025466Z"
    }
   },
   "source": [
    "# Define Training Task list for the pipe\n",
    "features = [('MEL_den', {'function':'frame', 'frames':5})]\n",
    "model = 'IsoFor'\n",
    "\n",
    "# Preprocessing pipe\n",
    "preprocessing = [\n",
    "    (PCA, {'n_components':64}),\n",
    "    (StandardScaler, {})\n",
    "]\n",
    "\n",
    "# uni_Model define\n",
    "# See also \\utility\\modeling\\pseudo_supervised_models.py form more uni models \n",
    "modeling = (uni_AutoEncoder, {'epochs':50})\n",
    "\n",
    "for machine in machines:\n",
    "    for SNR in SNRs:\n",
    "        for ID in IDs:\n",
    "            for feature in features:\n",
    "                task = {\n",
    "                'path_descr': find_data_file(SNR, machine, ID),\n",
    "                'feat':feature[1], \n",
    "                'feat_col':feature[0], \n",
    "                'SNR':SNR, \n",
    "                'machine':machine,\n",
    "                'ID':ID,\n",
    "                'BASE_FOLDER':BASE_FOLDER}\n",
    "\n",
    "                # append tasks\n",
    "                tasks.append(task)\n",
    "\n",
    "                # append pipes\n",
    "                pipes.append(Pipe(preprocessing, modeling, pseudo_sup=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation Forest on spectral data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T11:41:20.452067Z",
     "start_time": "2020-05-20T11:41:20.441129Z"
    }
   },
   "source": [
    "# Define Training Task list for the pipe\n",
    "features = [('MEL_den', {'function':'frame', 'frames':5})]\n",
    "model = 'IsoFor'\n",
    "\n",
    "# Preprocessing pipe\n",
    "preprocessing = [\n",
    "    (PCA, {'n_components':64}),\n",
    "    (StandardScaler, {})\n",
    "]\n",
    "\n",
    "# uni_Model define\n",
    "# See also \\utility\\modeling\\pseudo_supervised_models.py form more uni models \n",
    "modeling = (uni_IsolationForest, {'n_estimators':64, 'max_features':4})\n",
    "\n",
    "for machine in machines:\n",
    "    for SNR in SNRs:\n",
    "        for ID in IDs:\n",
    "            for feature in features:\n",
    "                task = {\n",
    "                'path_descr': find_data_file(SNR, machine, ID),\n",
    "                'feat':feature[1], \n",
    "                'feat_col':feature[0], \n",
    "                'SNR':SNR, \n",
    "                'machine':machine,\n",
    "                'ID':ID,\n",
    "                'BASE_FOLDER':BASE_FOLDER}\n",
    "\n",
    "                # append tasks\n",
    "                tasks.append(task)\n",
    "\n",
    "                # append pipes\n",
    "                pipes.append(Pipe(preprocessing, modeling, pseudo_sup=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation Forest on welch spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T14:09:20.612301Z",
     "start_time": "2020-05-20T14:09:20.443Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define Training Task list for the pipe\n",
    "features = [('PSD_raw', {'function':'channel'})]\n",
    "model = 'IsoFor'\n",
    "\n",
    "# Preprocessing pipe\n",
    "preprocessing = [\n",
    "    (StandardScaler, {})\n",
    "]\n",
    "\n",
    "# uni_Model define\n",
    "# See also \\utility\\modeling\\pseudo_supervised_models.py form more uni models \n",
    "modeling = (uni_IsolationForest, {'n_estimators':200, 'max_features':1})\n",
    "\n",
    "for machine in machines:\n",
    "    for SNR in SNRs:\n",
    "        for ID in IDs:\n",
    "            for feature in features:\n",
    "                task = {\n",
    "                'path_descr': find_data_file(SNR, machine, ID),\n",
    "                'feat':feature[1], \n",
    "                'feat_col':feature[0], \n",
    "                'SNR':SNR, \n",
    "                'machine':machine,\n",
    "                'ID':ID,\n",
    "                'BASE_FOLDER':BASE_FOLDER}\n",
    "\n",
    "                # append tasks\n",
    "                tasks.append(task)\n",
    "\n",
    "                # append pipes\n",
    "                pipes.append(Pipe(preprocessing, modeling, pseudo_sup=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:42:05.617953Z",
     "start_time": "2020-05-11T13:42:05.613964Z"
    }
   },
   "source": [
    "## Setting up pseudo supervised training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T08:52:28.203654Z",
     "start_time": "2020-05-20T08:52:28.189653Z"
    }
   },
   "source": [
    "# Define Training Task list for the pipe\n",
    "\n",
    "features = [('PSD_raw', {'function':'flat'})]\n",
    "model = 'SVM'\n",
    "\n",
    "# Preprocessing pipe\n",
    "preprocessing = [\n",
    "    (StandardScaler, {})\n",
    "]\n",
    "\n",
    "# uni_Model define\n",
    "# See also \\utility\\modeling\\pseudo_supervised_models.py form more uni models \n",
    "# also on how to use grid search (here not needed yet hence no augmentation feedback)\n",
    "modeling = (uni_svm, {'C': 0.1, 'degree':3,'kernel':'rbf'})\n",
    "\n",
    "\n",
    "for machine in machines:\n",
    "    for SNR in SNRs:\n",
    "        for ID in IDs:\n",
    "            for feature in features:\n",
    "                # check if model already exists\n",
    "                if not find_pipe(feature[0], SNR, machine, ID, model):\n",
    "                    task = {\n",
    "                    'path_descr': find_data_file(SNR, machine, ID),\n",
    "                    'feat':feature[1], \n",
    "                    'feat_col':feature[0], \n",
    "                    'SNR':SNR, \n",
    "                    'machine':machine,\n",
    "                    'ID':ID,\n",
    "                    'BASE_FOLDER':BASE_FOLDER}\n",
    "                    \n",
    "                    # append tasks\n",
    "                    tasks.append(task)\n",
    "                    \n",
    "                    # append pipes\n",
    "                    pipes.append(Pipe(preprocessing, modeling, pseudo_sup=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T14:09:20.613301Z",
     "start_time": "2020-05-20T14:09:20.447Z"
    }
   },
   "outputs": [],
   "source": [
    "pipes[0].run_pipe(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T14:09:20.614301Z",
     "start_time": "2020-05-20T14:09:20.456Z"
    }
   },
   "outputs": [],
   "source": [
    "tasks_failed = []\n",
    "for pipe, task in tqdm(zip(pipes, tasks), total=len(tasks)):\n",
    "    try:\n",
    "        pipe.run_pipe(task)\n",
    "    except:\n",
    "        tasks_failed.append(task)\n",
    "        print('Task failed \\n', task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:43:00.154076Z",
     "start_time": "2020-05-11T13:43:00.151082Z"
    }
   },
   "source": [
    "# Ensemble Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summery/ Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3.7 (mimii_base_TF2_GPU)",
   "language": "python",
   "name": "mimii-tf2-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
