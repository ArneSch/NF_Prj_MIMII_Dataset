{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:31:31.616054Z",
     "start_time": "2020-05-11T13:31:31.613337Z"
    }
   },
   "source": [
    "# About\n",
    "\n",
    "![prjpic](doc/media_main/story.png)\n",
    "\n",
    "This is the main demo of this repo, it is about a concept study on the MIMII dataset to detect anomalies of machines or machine parts like fans, slider, pump and valves by means of classic machine learning and deep learning methods.\n",
    "\n",
    "In runs through the essentials to demonstrate the steps\n",
    "* feature extraction\n",
    "* indvitual model training \n",
    "* ensamble building and varfication\n",
    "* summery and scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T11:53:08.568018Z",
     "start_time": "2020-05-20T11:53:08.511071Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load feature_extractor_mother\n",
      "load feature_extractor_mel_spectra\n",
      "load feature_extractor_psd\n",
      "load feature_extractor_ICA2\n",
      "load feature_extractore_pre_nnFilterDenoise\n",
      "load extractor_diagram_mother\n",
      "load Simple_FIR_HP\n",
      "load TimeSliceAppendActivation\n",
      "load load_data\n",
      "Load split_data\n",
      "Load anomaly_detection_models\n",
      "Load pseudo_supervised_models\n",
      "Load tensorflow models\n",
      "Load detection_pipe\n",
      "load # extractor diagram V1 essential\n",
      "load extractor_batch\n"
     ]
    }
   ],
   "source": [
    "#===============================================\n",
    "# Basic Imports\n",
    "BASE_FOLDER = './'\n",
    "TARGET_FOLDER_FE = r'\\dataset\\extdia_v1_essential' # output folder for ffeat. extraction\n",
    "# import the repo-local utility py files\n",
    "%run -i utility\\feature_extractor\\JupyterLoad_feature_extractor.py\n",
    "%run -i utility\\modeling\\JupyterLoad_modeling.py\n",
    "\n",
    "# feature extraction diagram\n",
    "%run -i feature_extraction_diagrams\\extdia_v1_essential\n",
    "%run -i utility\\extractor_batch.py\n",
    "\n",
    "# helper\n",
    "from tqdm.auto import tqdm\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "# sklearn \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "\n",
    "feat_ext_folder = os.path.abspath(BASE_FOLDER + TARGET_FOLDER_FE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General config\n",
    "\n",
    "To customize the following notebook execution, set the specifications here:\n",
    "\n",
    "The execution time on a local desktop PC for all four IDs of one machine and one SNR is circa 1 hour. We recommend only executing a set of 4 variations at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T11:53:08.575018Z",
     "start_time": "2020-05-20T11:53:08.570021Z"
    }
   },
   "outputs": [],
   "source": [
    "#===============================================\n",
    "# Possible variations\n",
    "# \n",
    "# SNRs = ['6dB', '0dB', 'min6dB']\n",
    "# machines = ['pump', 'valve', 'fan', 'slider']\n",
    "# IDs = ['00', '02', '04', '06']\n",
    "\n",
    "SNRs = ['6dB']\n",
    "machines = ['pump']\n",
    "IDs = ['00']\n",
    "\n",
    "# note: increase n_jobs to max. CPUs you have use all hyperthreading cores (there is no auto detect just now)\n",
    "n_jobs = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:39:10.674321Z",
     "start_time": "2020-05-11T13:39:10.671328Z"
    }
   },
   "source": [
    "## Utility wrapper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T11:53:08.588021Z",
     "start_time": "2020-05-20T11:53:08.578025Z"
    }
   },
   "outputs": [],
   "source": [
    "def feat_ext_process_set(FileFindDict, main_channel=0, sporadic=False, augment=False,FileCountLimit=None, n_jobs=4):\n",
    "    \n",
    "    if sporadic:\n",
    "        dt = 1 # 1 means time slicing \n",
    "    else:\n",
    "        dt = 0\n",
    "        \n",
    "    if augment:\n",
    "        ag = 0 # augment only normal operation\n",
    "    else:\n",
    "        ag = -2 # not existing class = no augment\n",
    "        \n",
    "    extractor_batch(base_folder= BASE_FOLDER, \n",
    "                    target_folder=TARGET_FOLDER_FE, \n",
    "                    extdia = extdia_v1_essential, \n",
    "                    FileFindDict = FileFindDict,\n",
    "                    n_jobs = n_jobs,\n",
    "                    target_class_map = {'abnormal':1, 'normal': 0},\n",
    "                    FileCountLimit = FileCountLimit,\n",
    "                    datset_folder_from_base = 'dataset',\n",
    "                    fHP=120,\n",
    "                    DeviceType=dt,\n",
    "                    main_channel = main_channel,\n",
    "                    augment=ag)\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T11:53:08.598019Z",
     "start_time": "2020-05-20T11:53:08.591018Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_data_file(SNR, machine, ID):\n",
    "    '''\n",
    "    function to find existing feature data files\n",
    "    '''\n",
    "    path = glob.glob(BASE_FOLDER \n",
    "              + '/dataset/extdia_v1_essential/{}{}{}_EDiaV1HP'.format(machine, SNR, ID) \n",
    "              + \"*pandaDisc*.pkl\", recursive=True)\n",
    "    \n",
    "    if len(path) == 0:\n",
    "        return None\n",
    "    elif len(path) == 1:\n",
    "        return path[0]\n",
    "    else:\n",
    "        raise Exception('More than one file found:', path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feat Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:40:58.788472Z",
     "start_time": "2020-05-11T13:40:58.786459Z"
    }
   },
   "source": [
    "## Confirm feature extraction diagram\n",
    "\n",
    "![exdia](doc/media_feature_extraction/exdia_v1_essential.png)\n",
    "in order to modify the diagram go the class definition:  /feature_extraction_diagrams/extdia_v1_essential.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note to the main_channel\n",
    "The main channel is picking one microphone out of the 8, \n",
    "this can be seen as if the demo version is strictly in working mono\n",
    "Or a DOA could be used to find the main direction see : feature_extraction_diagrams/A21_DirectionOfArrival_DOA/pyroomacustic_DOA.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T11:41:07.790440Z",
     "start_time": "2020-05-20T11:41:07.783440Z"
    },
    "scrolled": true
   },
   "source": [
    "if not find_data_file('6dB', 'pump', '02'):\n",
    "    ExampleFileFilter = {'SNR': ['6dB'],'machine': ['pump'],'ID': ['02']}\n",
    "    # create some \n",
    "    feat_ext_process_set(ExampleFileFilter,\n",
    "                        main_channel=2,\n",
    "                        sporadic = False,\n",
    "                        augment = True,\n",
    "                        FileCountLimit= 4,\n",
    "                        n_jobs=n_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spot Check the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T11:41:11.353733Z",
     "start_time": "2020-05-20T11:41:08.738734Z"
    }
   },
   "source": [
    "# This code reloads pkl files that have been stored\n",
    "# in the step above - notice only created files can be loaded\n",
    "# then a plot is made form n and n+1 output ports\n",
    "# this cell is ment as a spot check before running the batch that might,\n",
    "# take much more time !\n",
    "\n",
    "d_MEL_den = pickle.load( open( feat_ext_folder + r'\\pump6dB02_EDiaV1HPaug0_outpMEL_den.pkl', \"rb\" ))\n",
    "d_MEL_raw = pickle.load( open( feat_ext_folder + r'\\pump6dB02_EDiaV1HPaug0_outpMEL_raw.pkl', \"rb\" ))\n",
    "d_PSD_raw = pickle.load( open( feat_ext_folder + r'\\pump6dB02_EDiaV1HPaug0_outpPSD_raw.pkl', \"rb\" ))\n",
    "n1=2\n",
    "n2=7\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.subplot(321)\n",
    "feature_extractor_from_dict(d_MEL_raw[n1],BASE_FOLDER).plot(False)\n",
    "plt.subplot(322)\n",
    "feature_extractor_from_dict(d_MEL_raw[n2],BASE_FOLDER).plot(False)\n",
    "plt.subplot(323)\n",
    "feature_extractor_from_dict(d_MEL_den[n1],BASE_FOLDER).plot(False)\n",
    "plt.subplot(324)\n",
    "feature_extractor_from_dict(d_MEL_den[n2],BASE_FOLDER).plot(False)\n",
    "plt.subplot(325)\n",
    "feature_extractor_from_dict(d_PSD_raw[n1],BASE_FOLDER).plot(True)\n",
    "plt.subplot(326)\n",
    "feature_extractor_from_dict(d_PSD_raw[n2],BASE_FOLDER).plot(True)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch creation of feature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T11:53:08.610020Z",
     "start_time": "2020-05-20T11:53:08.600019Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the batch of feature data\n",
    "# note: there is still a deepcopy issue you may expirence memory leak : https://github.com/BA-HanseML/NF_Prj_MIMII_Dataset/issues/58\n",
    "for SNR in SNRs:\n",
    "    for machine in machines:\n",
    "        for ID in IDs:\n",
    "            # check if files already exist\n",
    "            if not find_data_file(SNR, machine, ID):\n",
    "                BatchFileFilter = {'SNR': SNR,'machine': machine,'ID': ID}\n",
    "                feat_ext_process_set(BatchFileFilter,\n",
    "                                    main_channel=2,\n",
    "                                    sporadic = False,\n",
    "                                    augment = True,\n",
    "                                    n_jobs=n_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T08:19:01.289568Z",
     "start_time": "2020-05-20T08:19:01.282569Z"
    }
   },
   "source": [
    "# PSD_raw_flat_6dB_pump_ID00_SVM\n",
    "\n",
    "def find_pipe(feature, SNR, machine, ID, model):\n",
    "    '''\n",
    "    function to find existing model pipe files\n",
    "    '''\n",
    "    path = glob.glob(BASE_FOLDER \n",
    "              + '/pipes/{}_*_{}_{}_ID{}_{}_*.pkl'.format(feature, SNR, machine, ID, model), recursive=True)\n",
    "    \n",
    "    if len(path) == 0:\n",
    "        return None\n",
    "    elif len(path) == 1:\n",
    "        return path[0]\n",
    "    else:\n",
    "        raise Exception('More than one file found:', path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T11:53:08.631018Z",
     "start_time": "2020-05-20T11:53:08.612022Z"
    }
   },
   "outputs": [],
   "source": [
    "class uni_Ensemble(object):\n",
    "    def __init__(SNR, machine, ID):\n",
    "        self.SNR = SNR\n",
    "        self.machine = machine\n",
    "        self.ID = ID\n",
    "        \n",
    "        self.tasks = [{\n",
    "                    'path_descr': find_data_file(SNR, machine, ID),\n",
    "                    'feat':feature[1], \n",
    "                    'feat_col':feature[0], \n",
    "                    'SNR':SNR, \n",
    "                    'machine':machine,\n",
    "                    'ID':ID,\n",
    "                    'BASE_FOLDER':BASE_FOLDER\n",
    "        } for feature in [('MEL_den', {'function':'frame', 'frames':3}), # Autoencoder MEL spectrum\n",
    "                          ('MEL_den', {'function':'frame', 'frames':5}), # Isolation Forest MEL spectrum\n",
    "                         ('PSD_raw', {'function':'flat'}), # Isolation Forest Welch method\n",
    "                         ('PSD_raw', {'function':'flat'})]] # SVM augmented Welch method\n",
    "        \n",
    "        self.pipes = [\n",
    "            Pipe(preprocessing_steps=[(PCA, {'n_components':64}),(StandardScaler, {})], \n",
    "                 modeling_step=(uni_AutoEncoder, {'epochs':50}),\n",
    "                 pseudo_sup=False), # Autoencoder MEL spectrum\n",
    "            \n",
    "            Pipe(preprocessing_steps=[(PCA, {'n_components':64}),(StandardScaler, {})], \n",
    "                 modeling_step=(uni_IsolationForest, {'n_estimators':64, 'max_features':4}),\n",
    "                 pseudo_sup=False), # Isolation Forest MEL spectrum\n",
    "            \n",
    "            Pipe(preprocessing_steps=[(StandardScaler, {})], \n",
    "                 modeling_step=(uni_IsolationForest, {'n_estimators':200, 'max_features':1}),\n",
    "                 pseudo_sup=False), # Isolation Forest Welch method\n",
    "            \n",
    "            Pipe(preprocessing_steps=[(StandardScaler, {})], \n",
    "                 modeling_step=(uni_svm, {'C': 0.1, 'degree':3,'kernel':'rbf'}),\n",
    "                 pseudo_sup=True), # SVM augmented Welch method\n",
    "        ]\n",
    "        \n",
    "    def fit():\n",
    "        for pipe, task in zip(self.pipes, self.tasks):\n",
    "            pipe.run_pipe(task)\n",
    "            \n",
    "    def predict():\n",
    "        pass\n",
    "    \n",
    "    def evaluate():\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up supervised training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T11:53:08.637019Z",
     "start_time": "2020-05-20T11:53:08.633019Z"
    }
   },
   "outputs": [],
   "source": [
    "#===============================================\n",
    "# Adding up all the tasks\n",
    "tasks = []\n",
    "pipes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T08:52:23.045467Z",
     "start_time": "2020-05-20T08:52:23.025466Z"
    }
   },
   "source": [
    "# Define Training Task list for the pipe\n",
    "features = [('MEL_den', {'function':'frame', 'frames':5})]\n",
    "model = 'IsoFor'\n",
    "\n",
    "# Preprocessing pipe\n",
    "preprocessing = [\n",
    "    (PCA, {'n_components':64}),\n",
    "    (StandardScaler, {})\n",
    "]\n",
    "\n",
    "# uni_Model define\n",
    "# See also \\utility\\modeling\\pseudo_supervised_models.py form more uni models \n",
    "modeling = (uni_AutoEncoder, {'epochs':50})\n",
    "\n",
    "for machine in machines:\n",
    "    for SNR in SNRs:\n",
    "        for ID in IDs:\n",
    "            for feature in features:\n",
    "                task = {\n",
    "                'path_descr': find_data_file(SNR, machine, ID),\n",
    "                'feat':feature[1], \n",
    "                'feat_col':feature[0], \n",
    "                'SNR':SNR, \n",
    "                'machine':machine,\n",
    "                'ID':ID,\n",
    "                'BASE_FOLDER':BASE_FOLDER}\n",
    "\n",
    "                # append tasks\n",
    "                tasks.append(task)\n",
    "\n",
    "                # append pipes\n",
    "                pipes.append(Pipe(preprocessing, modeling, pseudo_sup=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation Forest on spectral data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T11:41:20.452067Z",
     "start_time": "2020-05-20T11:41:20.441129Z"
    }
   },
   "source": [
    "# Define Training Task list for the pipe\n",
    "features = [('MEL_den', {'function':'frame', 'frames':5})]\n",
    "model = 'IsoFor'\n",
    "\n",
    "# Preprocessing pipe\n",
    "preprocessing = [\n",
    "    (PCA, {'n_components':64}),\n",
    "    (StandardScaler, {})\n",
    "]\n",
    "\n",
    "# uni_Model define\n",
    "# See also \\utility\\modeling\\pseudo_supervised_models.py form more uni models \n",
    "modeling = (uni_IsolationForest, {'n_estimators':64, 'max_features':4})\n",
    "\n",
    "for machine in machines:\n",
    "    for SNR in SNRs:\n",
    "        for ID in IDs:\n",
    "            for feature in features:\n",
    "                task = {\n",
    "                'path_descr': find_data_file(SNR, machine, ID),\n",
    "                'feat':feature[1], \n",
    "                'feat_col':feature[0], \n",
    "                'SNR':SNR, \n",
    "                'machine':machine,\n",
    "                'ID':ID,\n",
    "                'BASE_FOLDER':BASE_FOLDER}\n",
    "\n",
    "                # append tasks\n",
    "                tasks.append(task)\n",
    "\n",
    "                # append pipes\n",
    "                pipes.append(Pipe(preprocessing, modeling, pseudo_sup=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation Forest on welch spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T11:53:08.650020Z",
     "start_time": "2020-05-20T11:53:08.640018Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define Training Task list for the pipe\n",
    "features = [('PSD_raw', {'function':'channel'})]\n",
    "model = 'IsoFor'\n",
    "\n",
    "# Preprocessing pipe\n",
    "preprocessing = [\n",
    "    (StandardScaler, {})\n",
    "]\n",
    "\n",
    "# uni_Model define\n",
    "# See also \\utility\\modeling\\pseudo_supervised_models.py form more uni models \n",
    "modeling = (uni_IsolationForest, {'n_estimators':200, 'max_features':1})\n",
    "\n",
    "for machine in machines:\n",
    "    for SNR in SNRs:\n",
    "        for ID in IDs:\n",
    "            for feature in features:\n",
    "                task = {\n",
    "                'path_descr': find_data_file(SNR, machine, ID),\n",
    "                'feat':feature[1], \n",
    "                'feat_col':feature[0], \n",
    "                'SNR':SNR, \n",
    "                'machine':machine,\n",
    "                'ID':ID,\n",
    "                'BASE_FOLDER':BASE_FOLDER}\n",
    "\n",
    "                # append tasks\n",
    "                tasks.append(task)\n",
    "\n",
    "                # append pipes\n",
    "                pipes.append(Pipe(preprocessing, modeling, pseudo_sup=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:42:05.617953Z",
     "start_time": "2020-05-11T13:42:05.613964Z"
    }
   },
   "source": [
    "## Setting up pseudo supervised training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T08:52:28.203654Z",
     "start_time": "2020-05-20T08:52:28.189653Z"
    }
   },
   "source": [
    "# Define Training Task list for the pipe\n",
    "\n",
    "features = [('PSD_raw', {'function':'flat'})]\n",
    "model = 'SVM'\n",
    "\n",
    "# Preprocessing pipe\n",
    "preprocessing = [\n",
    "    (StandardScaler, {})\n",
    "]\n",
    "\n",
    "# uni_Model define\n",
    "# See also \\utility\\modeling\\pseudo_supervised_models.py form more uni models \n",
    "# also on how to use grid search (here not needed yet hence no augmentation feedback)\n",
    "modeling = (uni_svm, {'C': 0.1, 'degree':3,'kernel':'rbf'})\n",
    "\n",
    "\n",
    "for machine in machines:\n",
    "    for SNR in SNRs:\n",
    "        for ID in IDs:\n",
    "            for feature in features:\n",
    "                # check if model already exists\n",
    "                if not find_pipe(feature[0], SNR, machine, ID, model):\n",
    "                    task = {\n",
    "                    'path_descr': find_data_file(SNR, machine, ID),\n",
    "                    'feat':feature[1], \n",
    "                    'feat_col':feature[0], \n",
    "                    'SNR':SNR, \n",
    "                    'machine':machine,\n",
    "                    'ID':ID,\n",
    "                    'BASE_FOLDER':BASE_FOLDER}\n",
    "                    \n",
    "                    # append tasks\n",
    "                    tasks.append(task)\n",
    "                    \n",
    "                    # append pipes\n",
    "                    pipes.append(Pipe(preprocessing, modeling, pseudo_sup=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T11:53:10.018142Z",
     "start_time": "2020-05-20T11:53:08.662018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".//dataset/extdia_v1_essential\\pump6dB00_EDiaV1HPaug0_pandaDisc.pkl --> Done\n",
      "...loading data\n",
      "data loading completed\n",
      "\n",
      "...preprocessing data\n",
      "data preprocessing finished\n",
      "\n",
      "...fitting the model\n",
      "model fitted successfully\n",
      "\n",
      "...fitting the prediction scaler\n",
      "prediction scaler fitted successfully\n",
      "\n",
      "...evaluating model\n",
      "evaluation successfull, roc_auc: 0.9600958482077363\n",
      "pipe saved to pickle\n"
     ]
    }
   ],
   "source": [
    "pipes[0].run_pipe(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T11:53:11.404783Z",
     "start_time": "2020-05-20T11:53:10.020089Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac2df1264be40db9114c18073146b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".//dataset/extdia_v1_essential\\pump6dB00_EDiaV1HPaug0_pandaDisc.pkl --> Done\n",
      "...loading data\n",
      "data loading completed\n",
      "\n",
      "...preprocessing data\n",
      "data preprocessing finished\n",
      "\n",
      "...fitting the model\n",
      "model fitted successfully\n",
      "\n",
      "...fitting the prediction scaler\n",
      "prediction scaler fitted successfully\n",
      "\n",
      "...evaluating model\n",
      "evaluation successfull, roc_auc: 0.9600958482077363\n",
      "pipe saved to pickle\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tasks_failed = []\n",
    "for pipe, task in tqdm(zip(pipes, tasks), total=len(tasks)):\n",
    "    try:\n",
    "        pipe.run_pipe(task)\n",
    "    except:\n",
    "        tasks_failed.append(task)\n",
    "        print('Task failed \\n', task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:43:00.154076Z",
     "start_time": "2020-05-11T13:43:00.151082Z"
    }
   },
   "source": [
    "# Ensemble Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summery/ Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3.7 (mimii_base_TF2_GPU)",
   "language": "python",
   "name": "mimii-tf2-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
