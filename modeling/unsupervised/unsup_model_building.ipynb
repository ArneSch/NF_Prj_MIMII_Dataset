{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Structure-cheat-sheet\" data-toc-modified-id=\"Structure-cheat-sheet-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Structure cheat sheet</a></span></li><li><span><a href=\"#Data-structure\" data-toc-modified-id=\"Data-structure-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data structure</a></span></li><li><span><a href=\"#get-features\" data-toc-modified-id=\"get-features-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>get features</a></span></li><li><span><a href=\"#Exploration-of-non-spectral-Features\" data-toc-modified-id=\"Exploration-of-non-spectral-Features-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Exploration of non-spectral Features</a></span></li><li><span><a href=\"#Exploring-spectral-features\" data-toc-modified-id=\"Exploring-spectral-features-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Exploring spectral features</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics\n",
    "\n",
    "## Structure cheat sheet\n",
    "\n",
    "1. func: train data lead (following order)\n",
    "    1. read the descriptive dataframe from the feature-pipeline\n",
    "    2. extract feature from the feature-objects which are labeled train-dataset from dataframe\n",
    "    3. create numpy feature array for the processing pipeline\n",
    "2. preprocessing\n",
    "    1. Transformation (any combination of the following)\n",
    "        + log-transform\n",
    "        + PCA\n",
    "        + others\n",
    "    2. Scaling (one of the following)\n",
    "        + StandardScaler\n",
    "        + MinMaxScaler\n",
    "3. Unsupervised Clustering\n",
    "    1. Estimate initial hyperparameter\n",
    "    2. Create grid over various hyperparameters\n",
    "    3. Train all and choose the best according to metric\n",
    "    \n",
    "    \n",
    "in all steps the cluster-recorder object (possibly dataframe-row) will record all the meta-information like hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data structure\n",
    "\n",
    "There are multiple degrees of freedom in the data:\n",
    "\n",
    "1. Signal to noise ratio (SNR)\n",
    "2. Machine type\n",
    "    1. pump\n",
    "    2. fan\n",
    "    3. valve (solenoid)\n",
    "    4. slider\n",
    "3. Machine ID\n",
    "    1. four different machine IDs\n",
    "    \n",
    "The pipeline will be applied to fixed SNR, fixed machine type and fixed ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get features\n",
    "\n",
    "Get the descriptive dataframe for the features.\n",
    "\n",
    "The descriptive dataframe contains all IDs of the pump. We will focus on ID '00' for now since the modeling phase is seperated per SNR, per machine, per ID anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class: \n",
    "+ uni\\_\\<model\\>\n",
    "attributes:\n",
    "+ default threshold\n",
    "+ roc_auc\n",
    "methods:\n",
    "+ fit\n",
    "+ predict\n",
    "+ predict_score\n",
    "+ eval_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T08:10:08.129918Z",
     "start_time": "2020-05-09T08:09:59.346972Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "load feature_extractor_mother\nload feature_extractor_mel_spectra\nload feature_extractor_psd\nload feature_extractor_ICA2\nload feature_extractore_pre_nnFilterDenoise\nload extractor_diagram_mother\nload Simple_FIR_HP\nload TimeSliceAppendActivation\nload load_data\nLoad split_data\nLoad anomaly_detection_models\nLoad pseudo_supervised_models\nLoad tensorflow models\nLoad detection_pipe\n"
    }
   ],
   "source": [
    "#===============================================\n",
    "# Basic Imports\n",
    "BASE_FOLDER = '../../'\n",
    "%run -i ..\\..\\utility\\feature_extractor\\JupyterLoad_feature_extractor.py\n",
    "%run -i ..\\..\\utility\\modeling\\JupyterLoad_modeling.py\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from tqdm import tqdm\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exploration of the Dimensionality Reduction was already done in another notebook. We derived the following rules of thumb:\n",
    "\n",
    "1. PCA and ICA deliver almost the same results looking at the relative absolute error\n",
    "2. PCA is usually much faster\n",
    "3. on PSD and the ICA demix matrix, no dimensionality reduction is needed\n",
    "4. for a framed Mel-spectrum a number of components between 32 and 64 is a good measure. The resulting error is about 2-4%\n",
    "5. for a whole Mel-spectrum a number of components between 64 and 128 is advised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of non-spectral Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring spectral features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T08:10:23.723126Z",
     "start_time": "2020-05-09T08:10:23.544127Z"
    },
    "code_folding": [
     1,
     6,
     8,
     37
    ]
   },
   "outputs": [],
   "source": [
    "diagram = 'extdia_v1'\n",
    "machines = ['pump'#, 'fan', 'slider', 'valve'\n",
    "]\n",
    "SNRs = ['6dB', 'min6dB']\n",
    "IDs = ['00', '02']\n",
    "\n",
    "features = [('MEL_den', {'function':'frame', 'frames':5})\n",
    "            #, ('PSD_raw', {'function':'channel'})\n",
    "            ]\n",
    "\n",
    "tasks = [{\n",
    "        'path_descr':glob.glob(BASE_FOLDER \n",
    "                            + '/dataset/extdia_v1*/{}{}{}_EDiaV1'.format(machine, SNR, ID) \n",
    "                            + \"*pandaDisc*.pkl\", recursive=True)[0],\n",
    "        'feat':feature[1], \n",
    "        'feat_col':feature[0], \n",
    "        'SNR':SNR, \n",
    "        'machine':machine, \n",
    "        'ID':ID,\n",
    "        'BASE_FOLDER':BASE_FOLDER}\n",
    "        for machine in machines\n",
    "        for SNR in SNRs\n",
    "        for ID in IDs\n",
    "        for feature in features\n",
    "        ]\n",
    "\n",
    "preprocessing = [\n",
    "    (PCA, {'n_components':64}),\n",
    "    (StandardScaler, {})\n",
    "]\n",
    "\n",
    "modeling = (uni_AutoEncoder, {'epochs':50})\n",
    "\n",
    "pipes = [Pipe(preprocessing, modeling) for i in range(len(tasks))]\n",
    "\n",
    "# # create the threads\n",
    "# n_jobs = 4\n",
    "# worker_list = []\n",
    "# queue = Queue()\n",
    "# for worker in range(n_jobs):\n",
    "#     worker = PipeThread(queue)\n",
    "#     worker.daemon = True\n",
    "#     worker.start()\n",
    "#     worker_list.append(worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-09T08:10:27.301Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "../..//dataset\\extdia_v1\\pump6dB00_EDiaV1HPaug0_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nEpoch 1/50\n1042/1042 [==============================] - 14s 13ms/step - loss: 0.9413\nEpoch 2/50\n1042/1042 [==============================] - 9s 9ms/step - loss: 0.8958\nEpoch 3/50\n1042/1042 [==============================] - 10s 9ms/step - loss: 0.8691\nEpoch 4/50\n1042/1042 [==============================] - 9s 9ms/step - loss: 0.8479\nEpoch 5/50\n1042/1042 [==============================] - 10s 9ms/step - loss: 0.8299\nEpoch 6/50\n1042/1042 [==============================] - 9s 9ms/step - loss: 0.8151\nEpoch 7/50\n1042/1042 [==============================] - 10s 9ms/step - loss: 0.8022\nEpoch 8/50\n1042/1042 [==============================] - 11s 10ms/step - loss: 0.7934\nEpoch 9/50\n1042/1042 [==============================] - 10s 9ms/step - loss: 0.7844\nEpoch 10/50\n1042/1042 [==============================] - 9s 9ms/step - loss: 0.7772\nEpoch 11/50\n1042/1042 [==============================] - 10s 9ms/step - loss: 0.7711\nEpoch 12/50\n1042/1042 [==============================] - 9s 9ms/step - loss: 0.7654\nEpoch 13/50\n1042/1042 [==============================] - 9s 9ms/step - loss: 0.7605\nEpoch 14/50\n1042/1042 [==============================] - 9s 9ms/step - loss: 0.7560\nEpoch 15/50\n1042/1042 [==============================] - 9s 9ms/step - loss: 0.7519\nEpoch 16/50\n1042/1042 [==============================] - 9s 9ms/step - loss: 0.7486\nEpoch 17/50\n1042/1042 [==============================] - 9s 9ms/step - loss: 0.7458\nEpoch 18/50\n1042/1042 [==============================] - 9s 9ms/step - loss: 0.7427\nEpoch 19/50\n1042/1042 [==============================] - 9s 9ms/step - loss: 0.7402\nEpoch 20/50\n1042/1042 [==============================] - 9s 9ms/step - loss: 0.7375\nEpoch 21/50\n1042/1042 [==============================] - 9s 9ms/step - loss: 0.7354\nEpoch 22/50\n1042/1042 [==============================] - 10s 9ms/step - loss: 0.7330\nEpoch 23/50\n1042/1042 [==============================] - 10s 10ms/step - loss: 0.7312\nEpoch 24/50\n1042/1042 [==============================] - 9s 8ms/step - loss: 0.7294\nEpoch 25/50\n1042/1042 [==============================] - 8s 8ms/step - loss: 0.7277\nEpoch 26/50\n1042/1042 [==============================] - 8s 8ms/step - loss: 0.7266\nEpoch 27/50\n1042/1042 [==============================] - 8s 8ms/step - loss: 0.7245\nEpoch 28/50\n1042/1042 [==============================] - 9s 8ms/step - loss: 0.7234\nEpoch 29/50\n1042/1042 [==============================] - 10s 10ms/step - loss: 0.7226\nEpoch 30/50\n1042/1042 [==============================] - 9s 9ms/step - loss: 0.7219\nEpoch 31/50\n1042/1042 [==============================] - 8s 8ms/step - loss: 0.7203\nEpoch 32/50\n1042/1042 [==============================] - 8s 8ms/step - loss: 0.7189\nEpoch 33/50\n1042/1042 [==============================] - 9s 8ms/step - loss: 0.7191\nEpoch 34/50\n1042/1042 [==============================] - 8s 7ms/step - loss: 0.7183\nEpoch 35/50\n1042/1042 [==============================] - 8s 8ms/step - loss: 0.7169\nEpoch 36/50\n1042/1042 [==============================] - 8s 8ms/step - loss: 0.7156\nEpoch 37/50\n1042/1042 [==============================] - 8s 8ms/step - loss: 0.7153\nEpoch 38/50\n1042/1042 [==============================] - 8s 8ms/step - loss: 0.7151\nEpoch 39/50\n1042/1042 [==============================] - 8s 7ms/step - loss: 0.7137\nEpoch 40/50\n1042/1042 [==============================] - 8s 8ms/step - loss: 0.7139\nEpoch 41/50\n1042/1042 [==============================] - 9s 9ms/step - loss: 0.7126\nEpoch 42/50\n1042/1042 [==============================] - 8s 8ms/step - loss: 0.7104\nEpoch 43/50\n1042/1042 [==============================] - 8s 8ms/step - loss: 0.7092\nEpoch 44/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7094\nEpoch 45/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7081\nEpoch 46/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7073\nEpoch 47/50\n1042/1042 [==============================] - 8s 7ms/step - loss: 0.7063\nEpoch 48/50\n1042/1042 [==============================] - 8s 7ms/step - loss: 0.7059\nEpoch 49/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7054\nEpoch 50/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7044\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.9843152846869838\npipe saved to pickle\n../..//dataset\\extdia_v1\\pump6dB02_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nEpoch 1/50\n540/540 [==============================] - 6s 11ms/step - loss: 0.9595\nEpoch 2/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.9143\nEpoch 3/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.8795\nEpoch 4/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.8530\nEpoch 5/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.8338\nEpoch 6/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.8186\nEpoch 7/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.8050\nEpoch 8/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7913\nEpoch 9/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7796\nEpoch 10/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7698\nEpoch 11/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7616\nEpoch 12/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7548\nEpoch 13/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7489\nEpoch 14/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7431\nEpoch 15/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7391\nEpoch 16/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7355\nEpoch 17/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7318\nEpoch 18/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7283\nEpoch 19/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7247\nEpoch 20/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7223\nEpoch 21/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7197\nEpoch 22/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7172\nEpoch 23/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7151\nEpoch 24/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7134\nEpoch 25/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7113\nEpoch 26/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7094\nEpoch 27/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7075\nEpoch 28/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7056\nEpoch 29/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7046\nEpoch 30/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7032\nEpoch 31/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7020\nEpoch 32/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7009\nEpoch 33/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6996\nEpoch 34/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6986\nEpoch 35/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6974\nEpoch 36/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6964\nEpoch 37/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6953\nEpoch 38/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6943\nEpoch 39/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6932\nEpoch 40/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6924\nEpoch 41/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6914\nEpoch 42/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6905\nEpoch 43/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6897\nEpoch 44/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6889\nEpoch 45/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6879\nEpoch 46/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6871\nEpoch 47/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6861\nEpoch 48/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6859\nEpoch 49/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6850\nEpoch 50/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6844\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.9924827625606923\npipe saved to pickle\n../..//dataset\\extdia_v1\\pumpmin6dB00_EDiaV1HPaug0_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nEpoch 1/50\n1042/1042 [==============================] - 10s 9ms/step - loss: 0.9420\nEpoch 2/50\n1042/1042 [==============================] - 8s 7ms/step - loss: 0.8866\nEpoch 3/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.8508\nEpoch 4/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.8259\nEpoch 5/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.8081\nEpoch 6/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7965\nEpoch 7/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7878\nEpoch 8/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7794\nEpoch 9/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7721\nEpoch 10/50\n1042/1042 [==============================] - 8s 7ms/step - loss: 0.7648\nEpoch 11/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7586\nEpoch 12/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7539\nEpoch 13/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7500\nEpoch 14/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7454\nEpoch 15/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7421\nEpoch 16/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7392\nEpoch 17/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7351\nEpoch 18/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7324\nEpoch 19/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7302\nEpoch 20/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7274\nEpoch 21/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7260\nEpoch 22/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7237\nEpoch 23/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7225\nEpoch 24/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7207\nEpoch 25/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7195\nEpoch 26/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7173\nEpoch 27/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7162\nEpoch 28/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7150\nEpoch 29/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7143\nEpoch 30/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7139\nEpoch 31/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7127\nEpoch 32/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7118\nEpoch 33/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7101\nEpoch 34/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7095\nEpoch 35/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7079\nEpoch 36/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7071\nEpoch 37/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7055\nEpoch 38/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7044\nEpoch 39/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7031\nEpoch 40/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7015\nEpoch 41/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.7005\nEpoch 42/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.6991\nEpoch 43/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.6988\nEpoch 44/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.6981\nEpoch 45/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.6974\nEpoch 46/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.6958\nEpoch 47/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.6949\nEpoch 48/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.6938\nEpoch 49/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.6946\nEpoch 50/50\n1042/1042 [==============================] - 7s 7ms/step - loss: 0.6928\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.6990257249174503\npipe saved to pickle\n../..//dataset\\extdia_v1\\pumpmin6dB02_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nEpoch 1/50\n540/540 [==============================] - 6s 11ms/step - loss: 0.9505\nEpoch 2/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.8820\nEpoch 3/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.8397\nEpoch 4/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.8119\nEpoch 5/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7913\nEpoch 6/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7752\nEpoch 7/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7625\nEpoch 8/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7532\nEpoch 9/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7432\nEpoch 10/50\n540/540 [==============================] - 4s 8ms/step - loss: 0.7352\nEpoch 11/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7279\nEpoch 12/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7223\nEpoch 13/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7164\nEpoch 14/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7113\nEpoch 15/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7063\nEpoch 16/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.7013\nEpoch 17/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6976\nEpoch 18/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6945\nEpoch 19/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6908\nEpoch 20/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6873\nEpoch 21/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6837\nEpoch 22/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6807\nEpoch 23/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6780\nEpoch 24/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6758\nEpoch 25/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6731\nEpoch 26/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6714\nEpoch 27/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6695\nEpoch 28/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6676\nEpoch 29/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6657\nEpoch 30/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6637\nEpoch 31/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6623\nEpoch 32/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6600\nEpoch 33/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6576\nEpoch 34/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6562\nEpoch 35/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6551\nEpoch 36/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6543\nEpoch 37/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6536\nEpoch 38/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6521\nEpoch 39/50\n540/540 [==============================] - 4s 8ms/step - loss: 0.6506\nEpoch 40/50\n540/540 [==============================] - 4s 8ms/step - loss: 0.6493\nEpoch 41/50\n540/540 [==============================] - 4s 8ms/step - loss: 0.6482\nEpoch 42/50\n540/540 [==============================] - 4s 8ms/step - loss: 0.6473\nEpoch 43/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6462\nEpoch 44/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6456\nEpoch 45/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6450\nEpoch 46/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6444\nEpoch 47/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6431\nEpoch 48/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6421\nEpoch 49/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6413\nEpoch 50/50\n540/540 [==============================] - 4s 7ms/step - loss: 0.6410\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.7824495399501832\npipe saved to pickle\n"
    }
   ],
   "source": [
    "for pipe, task in (zip(pipes, tasks)):\n",
    "    pipe.run_pipe(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagram = 'extdia_v1'\n",
    "machines = ['valve', 'fan', 'slider', 'pump']\n",
    "SNRs = ['6dB', 'min6dB']\n",
    "IDs = ['00', '02']\n",
    "\n",
    "features = [('MEL_den', {'function':'frame', 'frames':3})\n",
    "            , ('PSD_raw', {'function':'channel'})\n",
    "            ]\n",
    "\n",
    "tasks = [{\n",
    "        'path_descr':glob.glob(BASE_FOLDER \n",
    "                            + '/dataset/extdia_v1*/{}{}{}_EDiaV1'.format(machine, SNR, ID) \n",
    "                            + \"*pandaDisc*.pkl\", recursive=True)[0],\n",
    "        'feat':feature[1], \n",
    "        'feat_col':feature[0], \n",
    "        'SNR':SNR, \n",
    "        'machine':machine, \n",
    "        'ID':ID,\n",
    "        'BASE_FOLDER':BASE_FOLDER}\n",
    "        for machine in machines\n",
    "        for SNR in SNRs\n",
    "        for ID in IDs\n",
    "        for feature in features\n",
    "        ]\n",
    "\n",
    "preprocessing = [\n",
    "    (PCA, {'n_components':64}),\n",
    "    (StandardScaler, {})\n",
    "]\n",
    "\n",
    "# modeling = (uni_AutoEncoder, {'epochs':50})\n",
    "modeling = (uni_IsolationForest, {'n_estimators':64, 'max_features':4})\n",
    "\n",
    "pipes = [Pipe(preprocessing, modeling) for i in range(len(tasks))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "../..//dataset\\extdia_v1\\valve6dB00_EDiaV1HPTsSl_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.8895119481315388\npipe saved to pickle\n../..//dataset\\extdia_v1\\valve6dB00_EDiaV1HPTsSl_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.49629263470093915\npipe saved to pickle\n../..//dataset\\extdia_v1\\valve6dB02_EDiaV1HPTsSl_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.8612501687270029\npipe saved to pickle\n../..//dataset\\extdia_v1\\valve6dB02_EDiaV1HPTsSl_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.9988888888888889\npipe saved to pickle\n../..//dataset\\extdia_v1\\valvemin6dB00_EDiaV1HPTsSl_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.641886730784859\npipe saved to pickle\n../..//dataset\\extdia_v1\\valvemin6dB00_EDiaV1HPTsSl_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.5109808629334087\npipe saved to pickle\n../..//dataset\\extdia_v1\\valvemin6dB02_EDiaV1HPTsSl_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.6455170391297306\npipe saved to pickle\n../..//dataset\\extdia_v1\\valvemin6dB02_EDiaV1HPTsSl_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.9550694444444444\npipe saved to pickle\n../..//dataset\\extdia_v1\\fan6dB00_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.8437206340301407\npipe saved to pickle\n../..//dataset\\extdia_v1\\fan6dB00_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.8847563221027595\npipe saved to pickle\n../..//dataset\\extdia_v1\\fan6dB02_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.9004583533324957\npipe saved to pickle\n../..//dataset\\extdia_v1\\fan6dB02_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.9997982635144047\npipe saved to pickle\n../..//dataset\\extdia_v1\\fanmin6dB00_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.543868565475433\npipe saved to pickle\n../..//dataset\\extdia_v1\\fanmin6dB00_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.5244583426401609\npipe saved to pickle\n../..//dataset\\extdia_v1\\fanmin6dB02_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.7339444815227341\npipe saved to pickle\n../..//dataset\\extdia_v1\\fanmin6dB02_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.6717126651717475\npipe saved to pickle\n../..//dataset\\extdia_v1\\slider6dB00_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.9247403579441076\npipe saved to pickle\n../..//dataset\\extdia_v1\\slider6dB00_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.9999999999999999\npipe saved to pickle\n../..//dataset\\extdia_v1\\slider6dB02_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.963983681598775\npipe saved to pickle\n../..//dataset\\extdia_v1\\slider6dB02_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.9996633421706014\npipe saved to pickle\n../..//dataset\\extdia_v1\\slidermin6dB00_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.789961340514151\npipe saved to pickle\n../..//dataset\\extdia_v1\\slidermin6dB00_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.9895688675672264\npipe saved to pickle\n../..//dataset\\extdia_v1\\slidermin6dB02_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.817196993949844\npipe saved to pickle\n../..//dataset\\extdia_v1\\slidermin6dB02_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.7655037944142854\npipe saved to pickle\n../..//dataset\\extdia_v1\\pump6dB00_EDiaV1HPaug0_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.9514711387572143\npipe saved to pickle\n../..//dataset\\extdia_v1\\pump6dB00_EDiaV1HPaug0_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.8604332730206856\npipe saved to pickle\n../..//dataset\\extdia_v1\\pump6dB02_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.9823876438320827\npipe saved to pickle\n../..//dataset\\extdia_v1\\pump6dB02_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.9846603360116873\npipe saved to pickle\n../..//dataset\\extdia_v1\\pumpmin6dB00_EDiaV1HPaug0_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.669897825587361\npipe saved to pickle\n../..//dataset\\extdia_v1\\pumpmin6dB00_EDiaV1HPaug0_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.8078634652061226\npipe saved to pickle\n../..//dataset\\extdia_v1\\pumpmin6dB02_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.7103939583034511\npipe saved to pickle\n../..//dataset\\extdia_v1\\pumpmin6dB02_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.8645402158915672\npipe saved to pickle\n"
    }
   ],
   "source": [
    "for pipe, task in (zip(pipes, tasks)):\n",
    "    pipe.run_pipe(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagram = 'extdia_v1'\n",
    "machines = ['valve', 'fan', 'slider', 'pump']\n",
    "SNRs = ['6dB', 'min6dB']\n",
    "IDs = ['00', '02']\n",
    "\n",
    "features = [('MEL_den', {'function':'frame', 'frames':3})\n",
    "            , ('PSD_raw', {'function':'channel'})\n",
    "            ]\n",
    "\n",
    "tasks = [{\n",
    "        'path_descr':glob.glob(BASE_FOLDER \n",
    "                            + '/dataset/extdia_v1*/{}{}{}_EDiaV1'.format(machine, SNR, ID) \n",
    "                            + \"*pandaDisc*.pkl\", recursive=True)[0],\n",
    "        'feat':feature[1], \n",
    "        'feat_col':feature[0], \n",
    "        'SNR':SNR, \n",
    "        'machine':machine, \n",
    "        'ID':ID,\n",
    "        'BASE_FOLDER':BASE_FOLDER}\n",
    "        for machine in machines\n",
    "        for SNR in SNRs\n",
    "        for ID in IDs\n",
    "        for feature in features\n",
    "        ]\n",
    "\n",
    "preprocessing = [\n",
    "    (PCA, {'n_components':64}),\n",
    "    (StandardScaler, {})\n",
    "]\n",
    "\n",
    "# modeling = (uni_AutoEncoder, {'epochs':50})\n",
    "modeling = (uni_IsolationForest, {'n_estimators':64, 'max_features':1})\n",
    "\n",
    "pipes = [Pipe(preprocessing, modeling) for i in range(len(tasks))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "../..//dataset\\extdia_v1\\valve6dB00_EDiaV1HPTsSl_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.7811116927265136\npipe saved to pickle\n../..//dataset\\extdia_v1\\valve6dB00_EDiaV1HPTsSl_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.6449403290728055\npipe saved to pickle\n../..//dataset\\extdia_v1\\valve6dB02_EDiaV1HPTsSl_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.843674398814919\npipe saved to pickle\n../..//dataset\\extdia_v1\\valve6dB02_EDiaV1HPTsSl_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.999375\npipe saved to pickle\n../..//dataset\\extdia_v1\\valvemin6dB00_EDiaV1HPTsSl_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.5830871845909427\npipe saved to pickle\n../..//dataset\\extdia_v1\\valvemin6dB00_EDiaV1HPTsSl_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.3817527010804322\npipe saved to pickle\n../..//dataset\\extdia_v1\\valvemin6dB02_EDiaV1HPTsSl_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.5824984060683134\npipe saved to pickle\n../..//dataset\\extdia_v1\\valvemin6dB02_EDiaV1HPTsSl_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.965625\npipe saved to pickle\n../..//dataset\\extdia_v1\\fan6dB00_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.8666546472166208\npipe saved to pickle\n../..//dataset\\extdia_v1\\fan6dB00_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.8629753273487917\npipe saved to pickle\n../..//dataset\\extdia_v1\\fan6dB02_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.8496744800360924\npipe saved to pickle\n../..//dataset\\extdia_v1\\fan6dB02_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 1.0\npipe saved to pickle\n../..//dataset\\extdia_v1\\fanmin6dB00_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.5220135339297457\npipe saved to pickle\n../..//dataset\\extdia_v1\\fanmin6dB00_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.5150408393651636\npipe saved to pickle\n../..//dataset\\extdia_v1\\fanmin6dB02_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.6971676778337792\npipe saved to pickle\n../..//dataset\\extdia_v1\\fanmin6dB02_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.7031835569246048\npipe saved to pickle\n../..//dataset\\extdia_v1\\slider6dB00_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.888445181835058\npipe saved to pickle\n../..//dataset\\extdia_v1\\slider6dB00_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 1.0\npipe saved to pickle\n../..//dataset\\extdia_v1\\slider6dB02_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.9461425246544428\npipe saved to pickle\n../..//dataset\\extdia_v1\\slider6dB02_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.9909944030635863\npipe saved to pickle\n../..//dataset\\extdia_v1\\slidermin6dB00_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.7041108333787425\npipe saved to pickle\n../..//dataset\\extdia_v1\\slidermin6dB00_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.9918570887514203\npipe saved to pickle\n../..//dataset\\extdia_v1\\slidermin6dB02_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.7714140692003355\npipe saved to pickle\n../..//dataset\\extdia_v1\\slidermin6dB02_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.7916228310117971\npipe saved to pickle\n../..//dataset\\extdia_v1\\pump6dB00_EDiaV1HPaug0_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.8572344625626132\npipe saved to pickle\n../..//dataset\\extdia_v1\\pump6dB00_EDiaV1HPaug0_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.8982835346471709\npipe saved to pickle\n../..//dataset\\extdia_v1\\pump6dB02_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.9552816086283622\npipe saved to pickle\n../..//dataset\\extdia_v1\\pump6dB02_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.9933446960473988\npipe saved to pickle\n../..//dataset\\extdia_v1\\pumpmin6dB00_EDiaV1HPaug0_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.6428190418090572\npipe saved to pickle\n../..//dataset\\extdia_v1\\pumpmin6dB00_EDiaV1HPaug0_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.8098195510782923\npipe saved to pickle\n../..//dataset\\extdia_v1\\pumpmin6dB02_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.6502888852156473\npipe saved to pickle\n../..//dataset\\extdia_v1\\pumpmin6dB02_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.8885642399155912\npipe saved to pickle\n"
    }
   ],
   "source": [
    "for pipe, task in (zip(pipes, tasks)):\n",
    "    pipe.run_pipe(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagram = 'extdia_v1'\n",
    "machines = ['valve', 'fan', 'slider', 'pump']\n",
    "SNRs = ['6dB', 'min6dB']\n",
    "IDs = ['00', '02']\n",
    "\n",
    "features = [#('MEL_den', {'function':'frame', 'frames':3})\n",
    "            ('PSD_raw', {'function':'channel'})\n",
    "            ]\n",
    "\n",
    "tasks = [{\n",
    "        'path_descr':glob.glob(BASE_FOLDER \n",
    "                            + '/dataset/extdia_v1*/{}{}{}_EDiaV1'.format(machine, SNR, ID) \n",
    "                            + \"*pandaDisc*.pkl\", recursive=True)[0],\n",
    "        'feat':feature[1], \n",
    "        'feat_col':feature[0], \n",
    "        'SNR':SNR, \n",
    "        'machine':machine, \n",
    "        'ID':ID,\n",
    "        'BASE_FOLDER':BASE_FOLDER}\n",
    "        for machine in machines\n",
    "        for SNR in SNRs\n",
    "        for ID in IDs\n",
    "        for feature in features\n",
    "        ]\n",
    "\n",
    "preprocessing = [\n",
    "    #(PCA, {'n_components':64}),\n",
    "    (StandardScaler, {})\n",
    "]\n",
    "\n",
    "modeling = (uni_AutoEncoder, {'epochs':50, 'inter_layers':[(tf.keras.layers.Dense, {'units':128, 'activation':tf.nn.relu}),\n",
    "                               (tf.keras.layers.Dense, {'units':64, 'activation':tf.nn.relu}),\n",
    "                               (tf.keras.layers.Dense, {'units':8, 'activation':tf.nn.relu}),\n",
    "                               (tf.keras.layers.Dense, {'units':64, 'activation':tf.nn.relu}),\n",
    "                               (tf.keras.layers.Dense, {'units':128, 'activation':tf.nn.relu})]})\n",
    "# modeling = (uni_IsolationForest, {'n_estimators':64, 'max_features':1})\n",
    "\n",
    "pipes = [Pipe(preprocessing, modeling) for i in range(len(tasks))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "============================] - 0s 71ms/step - loss: 0.6220\nEpoch 15/50\n1/1 [==============================] - 0s 69ms/step - loss: 0.6033\nEpoch 16/50\n1/1 [==============================] - 0s 71ms/step - loss: 0.5813\nEpoch 17/50\n1/1 [==============================] - 0s 71ms/step - loss: 0.5672\nEpoch 18/50\n1/1 [==============================] - 0s 72ms/step - loss: 0.5498\nEpoch 19/50\n1/1 [==============================] - 0s 75ms/step - loss: 0.5384\nEpoch 20/50\n1/1 [==============================] - 0s 69ms/step - loss: 0.5269\nEpoch 21/50\n1/1 [==============================] - 0s 68ms/step - loss: 0.5174\nEpoch 22/50\n1/1 [==============================] - 0s 68ms/step - loss: 0.5090\nEpoch 23/50\n1/1 [==============================] - 0s 68ms/step - loss: 0.5011\nEpoch 24/50\n1/1 [==============================] - 0s 70ms/step - loss: 0.4938\nEpoch 25/50\n1/1 [==============================] - 0s 66ms/step - loss: 0.4860\nEpoch 26/50\n1/1 [==============================] - 0s 67ms/step - loss: 0.4799\nEpoch 27/50\n1/1 [==============================] - 0s 66ms/step - loss: 0.4731\nEpoch 28/50\n1/1 [==============================] - 0s 70ms/step - loss: 0.4673\nEpoch 29/50\n1/1 [==============================] - 0s 74ms/step - loss: 0.4614\nEpoch 30/50\n1/1 [==============================] - 0s 70ms/step - loss: 0.4553\nEpoch 31/50\n1/1 [==============================] - 0s 69ms/step - loss: 0.4506\nEpoch 32/50\n1/1 [==============================] - 0s 71ms/step - loss: 0.4446\nEpoch 33/50\n1/1 [==============================] - 0s 69ms/step - loss: 0.4401\nEpoch 34/50\n1/1 [==============================] - 0s 72ms/step - loss: 0.4349\nEpoch 35/50\n1/1 [==============================] - 0s 69ms/step - loss: 0.4297\nEpoch 36/50\n1/1 [==============================] - 0s 73ms/step - loss: 0.4251\nEpoch 37/50\n1/1 [==============================] - 0s 67ms/step - loss: 0.4196\nEpoch 38/50\n1/1 [==============================] - 0s 68ms/step - loss: 0.4148\nEpoch 39/50\n1/1 [==============================] - 0s 69ms/step - loss: 0.4099\nEpoch 40/50\n1/1 [==============================] - 0s 66ms/step - loss: 0.4054\nEpoch 41/50\n1/1 [==============================] - 0s 65ms/step - loss: 0.4008\nEpoch 42/50\n1/1 [==============================] - 0s 71ms/step - loss: 0.3963\nEpoch 43/50\n1/1 [==============================] - 0s 76ms/step - loss: 0.3921\nEpoch 44/50\n1/1 [==============================] - 0s 69ms/step - loss: 0.3881\nEpoch 45/50\n1/1 [==============================] - 0s 71ms/step - loss: 0.3860\nEpoch 46/50\n1/1 [==============================] - 0s 70ms/step - loss: 0.3844\nEpoch 47/50\n1/1 [==============================] - 0s 69ms/step - loss: 0.3791\nEpoch 48/50\n1/1 [==============================] - 0s 67ms/step - loss: 0.3739\nEpoch 49/50\n1/1 [==============================] - 0s 68ms/step - loss: 0.3720\nEpoch 50/50\n1/1 [==============================] - 0s 74ms/step - loss: 0.3666\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.8485741138184012\npipe saved to pickle\n../..//dataset\\extdia_v1\\pump6dB00_EDiaV1HPaug0_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nEpoch 1/50\n4/4 [==============================] - 2s 487ms/step - loss: 0.9767\nEpoch 2/50\n4/4 [==============================] - 0s 30ms/step - loss: 0.9642\nEpoch 3/50\n4/4 [==============================] - 0s 28ms/step - loss: 0.8499\nEpoch 4/50\n4/4 [==============================] - 0s 28ms/step - loss: 0.8333\nEpoch 5/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.7896\nEpoch 6/50\n4/4 [==============================] - 0s 30ms/step - loss: 0.7249\nEpoch 7/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.6606\nEpoch 8/50\n4/4 [==============================] - 0s 28ms/step - loss: 0.5950\nEpoch 9/50\n4/4 [==============================] - 0s 27ms/step - loss: 0.6024\nEpoch 10/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.5661\nEpoch 11/50\n4/4 [==============================] - 0s 28ms/step - loss: 0.5513\nEpoch 12/50\n4/4 [==============================] - 0s 28ms/step - loss: 0.5303\nEpoch 13/50\n4/4 [==============================] - 0s 28ms/step - loss: 0.4920\nEpoch 14/50\n4/4 [==============================] - 0s 28ms/step - loss: 0.4866\nEpoch 15/50\n4/4 [==============================] - 0s 30ms/step - loss: 0.4945\nEpoch 16/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.4914\nEpoch 17/50\n4/4 [==============================] - 0s 28ms/step - loss: 0.4494\nEpoch 18/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.4569\nEpoch 19/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.3860\nEpoch 20/50\n4/4 [==============================] - 0s 28ms/step - loss: 0.4151\nEpoch 21/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.4212\nEpoch 22/50\n4/4 [==============================] - 0s 28ms/step - loss: 0.4083\nEpoch 23/50\n4/4 [==============================] - 0s 30ms/step - loss: 0.3871\nEpoch 24/50\n4/4 [==============================] - 0s 28ms/step - loss: 0.3827\nEpoch 25/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.3613\nEpoch 26/50\n4/4 [==============================] - 0s 30ms/step - loss: 0.3506\nEpoch 27/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.3521\nEpoch 28/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.3601\nEpoch 29/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.3671\nEpoch 30/50\n4/4 [==============================] - 0s 30ms/step - loss: 0.3572\nEpoch 31/50\n4/4 [==============================] - 0s 31ms/step - loss: 0.3198\nEpoch 32/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.3491\nEpoch 33/50\n4/4 [==============================] - 0s 30ms/step - loss: 0.3631\nEpoch 34/50\n4/4 [==============================] - 0s 31ms/step - loss: 0.3450\nEpoch 35/50\n4/4 [==============================] - 0s 27ms/step - loss: 0.3100\nEpoch 36/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.3147\nEpoch 37/50\n4/4 [==============================] - 0s 30ms/step - loss: 0.3222\nEpoch 38/50\n4/4 [==============================] - 0s 30ms/step - loss: 0.3198\nEpoch 39/50\n4/4 [==============================] - 0s 28ms/step - loss: 0.3130\nEpoch 40/50\n4/4 [==============================] - 0s 30ms/step - loss: 0.2855\nEpoch 41/50\n4/4 [==============================] - 0s 32ms/step - loss: 0.2861\nEpoch 42/50\n4/4 [==============================] - 0s 33ms/step - loss: 0.2779\nEpoch 43/50\n4/4 [==============================] - 0s 32ms/step - loss: 0.2667\nEpoch 44/50\n4/4 [==============================] - 0s 33ms/step - loss: 0.2621\nEpoch 45/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.2604\nEpoch 46/50\n4/4 [==============================] - 0s 28ms/step - loss: 0.2506\nEpoch 47/50\n4/4 [==============================] - 0s 30ms/step - loss: 0.2453\nEpoch 48/50\n4/4 [==============================] - 0s 30ms/step - loss: 0.2355\nEpoch 49/50\n4/4 [==============================] - 0s 30ms/step - loss: 0.2383\nEpoch 50/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.2383\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.910068952026994\npipe saved to pickle\n../..//dataset\\extdia_v1\\pump6dB02_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nEpoch 1/50\n2/2 [==============================] - 1s 680ms/step - loss: 1.0086\nEpoch 2/50\n2/2 [==============================] - 0s 44ms/step - loss: 0.9681\nEpoch 3/50\n2/2 [==============================] - 0s 43ms/step - loss: 0.9735\nEpoch 4/50\n2/2 [==============================] - 0s 44ms/step - loss: 0.9124\nEpoch 5/50\n2/2 [==============================] - 0s 42ms/step - loss: 0.8215\nEpoch 6/50\n2/2 [==============================] - 0s 45ms/step - loss: 0.7828\nEpoch 7/50\n2/2 [==============================] - 0s 44ms/step - loss: 0.7318\nEpoch 8/50\n2/2 [==============================] - 0s 48ms/step - loss: 0.6850\nEpoch 9/50\n2/2 [==============================] - 0s 43ms/step - loss: 0.6385\nEpoch 10/50\n2/2 [==============================] - 0s 43ms/step - loss: 0.6121\nEpoch 11/50\n2/2 [==============================] - 0s 46ms/step - loss: 0.5895\nEpoch 12/50\n2/2 [==============================] - 0s 43ms/step - loss: 0.5677\nEpoch 13/50\n2/2 [==============================] - 0s 43ms/step - loss: 0.5519\nEpoch 14/50\n2/2 [==============================] - 0s 44ms/step - loss: 0.5282\nEpoch 15/50\n2/2 [==============================] - 0s 52ms/step - loss: 0.5420\nEpoch 16/50\n2/2 [==============================] - 0s 54ms/step - loss: 0.5212\nEpoch 17/50\n2/2 [==============================] - 0s 52ms/step - loss: 0.4987\nEpoch 18/50\n2/2 [==============================] - 0s 53ms/step - loss: 0.4987\nEpoch 19/50\n2/2 [==============================] - 0s 45ms/step - loss: 0.5027\nEpoch 20/50\n2/2 [==============================] - 0s 43ms/step - loss: 0.4855\nEpoch 21/50\n2/2 [==============================] - 0s 43ms/step - loss: 0.4885\nEpoch 22/50\n2/2 [==============================] - 0s 48ms/step - loss: 0.4579\nEpoch 23/50\n2/2 [==============================] - 0s 44ms/step - loss: 0.4718\nEpoch 24/50\n2/2 [==============================] - 0s 43ms/step - loss: 0.4413\nEpoch 25/50\n2/2 [==============================] - 0s 42ms/step - loss: 0.4456\nEpoch 26/50\n2/2 [==============================] - 0s 47ms/step - loss: 0.4578\nEpoch 27/50\n2/2 [==============================] - 0s 48ms/step - loss: 0.4390\nEpoch 28/50\n2/2 [==============================] - 0s 44ms/step - loss: 0.4392\nEpoch 29/50\n2/2 [==============================] - 0s 43ms/step - loss: 0.4340\nEpoch 30/50\n2/2 [==============================] - 0s 43ms/step - loss: 0.4281\nEpoch 31/50\n2/2 [==============================] - 0s 43ms/step - loss: 0.4161\nEpoch 32/50\n2/2 [==============================] - 0s 41ms/step - loss: 0.4312\nEpoch 33/50\n2/2 [==============================] - 0s 45ms/step - loss: 0.4206\nEpoch 34/50\n2/2 [==============================] - 0s 41ms/step - loss: 0.3976\nEpoch 35/50\n2/2 [==============================] - 0s 44ms/step - loss: 0.4148\nEpoch 36/50\n2/2 [==============================] - 0s 42ms/step - loss: 0.4053\nEpoch 37/50\n2/2 [==============================] - 0s 45ms/step - loss: 0.3913\nEpoch 38/50\n2/2 [==============================] - 0s 44ms/step - loss: 0.4116\nEpoch 39/50\n2/2 [==============================] - 0s 47ms/step - loss: 0.3820\nEpoch 40/50\n2/2 [==============================] - 0s 44ms/step - loss: 0.3965\nEpoch 41/50\n2/2 [==============================] - 0s 42ms/step - loss: 0.3849\nEpoch 42/50\n2/2 [==============================] - 0s 44ms/step - loss: 0.3853\nEpoch 43/50\n2/2 [==============================] - 0s 40ms/step - loss: 0.3785\nEpoch 44/50\n2/2 [==============================] - 0s 45ms/step - loss: 0.3902\nEpoch 45/50\n2/2 [==============================] - 0s 43ms/step - loss: 0.3749\nEpoch 46/50\n2/2 [==============================] - 0s 44ms/step - loss: 0.3782\nEpoch 47/50\n2/2 [==============================] - 0s 42ms/step - loss: 0.3699\nEpoch 48/50\n2/2 [==============================] - 0s 43ms/step - loss: 0.3616\nEpoch 49/50\n2/2 [==============================] - 0s 42ms/step - loss: 0.3682\nEpoch 50/50\n2/2 [==============================] - 0s 45ms/step - loss: 0.3820\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.8712766820874929\npipe saved to pickle\n../..//dataset\\extdia_v1\\pumpmin6dB00_EDiaV1HPaug0_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nEpoch 1/50\n4/4 [==============================] - 1s 346ms/step - loss: 0.9426\nEpoch 2/50\n4/4 [==============================] - 0s 28ms/step - loss: 0.9708\nEpoch 3/50\n4/4 [==============================] - 0s 28ms/step - loss: 0.7719\nEpoch 4/50\n4/4 [==============================] - 0s 31ms/step - loss: 0.8313\nEpoch 5/50\n4/4 [==============================] - 0s 35ms/step - loss: 0.7697\nEpoch 6/50\n4/4 [==============================] - 0s 33ms/step - loss: 0.7333\nEpoch 7/50\n4/4 [==============================] - 0s 33ms/step - loss: 0.6253\nEpoch 8/50\n4/4 [==============================] - 0s 34ms/step - loss: 0.6977\nEpoch 9/50\n4/4 [==============================] - 0s 31ms/step - loss: 0.6047\nEpoch 10/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.6056\nEpoch 11/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.5771\nEpoch 12/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.5329\nEpoch 13/50\n4/4 [==============================] - 0s 28ms/step - loss: 0.4944\nEpoch 14/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.4934\nEpoch 15/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.4404\nEpoch 16/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.4571\nEpoch 17/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.4836\nEpoch 18/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.4658\nEpoch 19/50\n4/4 [==============================] - 0s 28ms/step - loss: 0.4445\nEpoch 20/50\n4/4 [==============================] - 0s 28ms/step - loss: 0.4256\nEpoch 21/50\n4/4 [==============================] - 0s 30ms/step - loss: 0.4198\nEpoch 22/50\n4/4 [==============================] - 0s 31ms/step - loss: 0.4005\nEpoch 23/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.3995\nEpoch 24/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.4065\nEpoch 25/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.3832\nEpoch 26/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.3703\nEpoch 27/50\n4/4 [==============================] - 0s 28ms/step - loss: 0.3809\nEpoch 28/50\n4/4 [==============================] - 0s 28ms/step - loss: 0.4047\nEpoch 29/50\n4/4 [==============================] - 0s 30ms/step - loss: 0.4013\nEpoch 30/50\n4/4 [==============================] - 0s 28ms/step - loss: 0.3773\nEpoch 31/50\n4/4 [==============================] - 0s 30ms/step - loss: 0.3726\nEpoch 32/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.3741\nEpoch 33/50\n4/4 [==============================] - 0s 30ms/step - loss: 0.3485\nEpoch 34/50\n4/4 [==============================] - 0s 28ms/step - loss: 0.4305\nEpoch 35/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.4205\nEpoch 36/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.4168\nEpoch 37/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.3749\nEpoch 38/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.4086\nEpoch 39/50\n4/4 [==============================] - 0s 31ms/step - loss: 0.3855\nEpoch 40/50\n4/4 [==============================] - 0s 28ms/step - loss: 0.3899\nEpoch 41/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.3703\nEpoch 42/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.3350\nEpoch 43/50\n4/4 [==============================] - 0s 30ms/step - loss: 0.3479\nEpoch 44/50\n4/4 [==============================] - 0s 28ms/step - loss: 0.3329\nEpoch 45/50\n4/4 [==============================] - 0s 28ms/step - loss: 0.3309\nEpoch 46/50\n4/4 [==============================] - 0s 30ms/step - loss: 0.3242\nEpoch 47/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.3313\nEpoch 48/50\n4/4 [==============================] - 0s 29ms/step - loss: 0.3236\nEpoch 49/50\n4/4 [==============================] - 0s 31ms/step - loss: 0.3114\nEpoch 50/50\n4/4 [==============================] - 0s 28ms/step - loss: 0.3095\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.8286957797447307\npipe saved to pickle\n../..//dataset\\extdia_v1\\pumpmin6dB02_EDiaV1HP_pandaDisc.pkl --> Done\n...loading data\ndata loading completed\n\n...preprocessing data\ndata preprocessing finished\n\n...fitting the model\nEpoch 1/50\n2/2 [==============================] - 1s 734ms/step - loss: 0.9941\nEpoch 2/50\n2/2 [==============================] - 0s 43ms/step - loss: 0.8633\nEpoch 3/50\n2/2 [==============================] - 0s 44ms/step - loss: 0.8547\nEpoch 4/50\n2/2 [==============================] - 0s 41ms/step - loss: 0.8087\nEpoch 5/50\n2/2 [==============================] - 0s 49ms/step - loss: 0.7765\nEpoch 6/50\n2/2 [==============================] - 0s 42ms/step - loss: 0.7024\nEpoch 7/50\n2/2 [==============================] - 0s 45ms/step - loss: 0.6703\nEpoch 8/50\n2/2 [==============================] - 0s 43ms/step - loss: 0.5949\nEpoch 9/50\n2/2 [==============================] - 0s 45ms/step - loss: 0.5771\nEpoch 10/50\n2/2 [==============================] - 0s 45ms/step - loss: 0.5591\nEpoch 11/50\n2/2 [==============================] - 0s 45ms/step - loss: 0.5471\nEpoch 12/50\n2/2 [==============================] - 0s 44ms/step - loss: 0.4962\nEpoch 13/50\n2/2 [==============================] - 0s 44ms/step - loss: 0.4904\nEpoch 14/50\n2/2 [==============================] - 0s 41ms/step - loss: 0.5186\nEpoch 15/50\n2/2 [==============================] - 0s 43ms/step - loss: 0.4568\nEpoch 16/50\n2/2 [==============================] - 0s 44ms/step - loss: 0.5071\nEpoch 17/50\n2/2 [==============================] - 0s 42ms/step - loss: 0.4984\nEpoch 18/50\n2/2 [==============================] - 0s 42ms/step - loss: 0.4354\nEpoch 19/50\n2/2 [==============================] - 0s 42ms/step - loss: 0.4605\nEpoch 20/50\n2/2 [==============================] - 0s 44ms/step - loss: 0.4636\nEpoch 21/50\n2/2 [==============================] - 0s 43ms/step - loss: 0.4607\nEpoch 22/50\n2/2 [==============================] - 0s 44ms/step - loss: 0.4528\nEpoch 23/50\n2/2 [==============================] - 0s 43ms/step - loss: 0.4336\nEpoch 24/50\n2/2 [==============================] - 0s 46ms/step - loss: 0.4315\nEpoch 25/50\n2/2 [==============================] - 0s 43ms/step - loss: 0.4181\nEpoch 26/50\n2/2 [==============================] - 0s 45ms/step - loss: 0.4312\nEpoch 27/50\n2/2 [==============================] - 0s 43ms/step - loss: 0.4225\nEpoch 28/50\n2/2 [==============================] - 0s 44ms/step - loss: 0.4123\nEpoch 29/50\n2/2 [==============================] - 0s 41ms/step - loss: 0.3804\nEpoch 30/50\n2/2 [==============================] - 0s 47ms/step - loss: 0.3884\nEpoch 31/50\n2/2 [==============================] - 0s 42ms/step - loss: 0.3980\nEpoch 32/50\n2/2 [==============================] - 0s 46ms/step - loss: 0.3754\nEpoch 33/50\n2/2 [==============================] - 0s 44ms/step - loss: 0.4056\nEpoch 34/50\n2/2 [==============================] - 0s 44ms/step - loss: 0.4022\nEpoch 35/50\n2/2 [==============================] - 0s 47ms/step - loss: 0.3656\nEpoch 36/50\n2/2 [==============================] - 0s 45ms/step - loss: 0.3726\nEpoch 37/50\n2/2 [==============================] - 0s 45ms/step - loss: 0.3446\nEpoch 38/50\n2/2 [==============================] - 0s 41ms/step - loss: 0.3765\nEpoch 39/50\n2/2 [==============================] - 0s 45ms/step - loss: 0.3889\nEpoch 40/50\n2/2 [==============================] - 0s 43ms/step - loss: 0.3629\nEpoch 41/50\n2/2 [==============================] - 0s 41ms/step - loss: 0.3638\nEpoch 42/50\n2/2 [==============================] - 0s 42ms/step - loss: 0.3570\nEpoch 43/50\n2/2 [==============================] - 0s 48ms/step - loss: 0.3592\nEpoch 44/50\n2/2 [==============================] - 0s 47ms/step - loss: 0.3358\nEpoch 45/50\n2/2 [==============================] - 0s 43ms/step - loss: 0.3930\nEpoch 46/50\n2/2 [==============================] - 0s 43ms/step - loss: 0.3653\nEpoch 47/50\n2/2 [==============================] - 0s 41ms/step - loss: 0.3655\nEpoch 48/50\n2/2 [==============================] - 0s 46ms/step - loss: 0.3603\nEpoch 49/50\n2/2 [==============================] - 0s 41ms/step - loss: 0.3509\nEpoch 50/50\n2/2 [==============================] - 0s 44ms/step - loss: 0.3762\nmodel fitted successfully\n\n...evaluating model\nevaluation successfull, roc_auc: 0.768038308578849\npipe saved to pickle\n"
    }
   ],
   "source": [
    "for pipe, task in (zip(pipes, tasks)):\n",
    "    pipe.run_pipe(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3.7 (mimii_base_TF2_GPU)",
   "language": "python",
   "name": "mimii-tf2-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}