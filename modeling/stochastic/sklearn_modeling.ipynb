{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Structure-cheat-sheet\" data-toc-modified-id=\"Structure-cheat-sheet-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Structure cheat sheet</a></span></li><li><span><a href=\"#Data-structure\" data-toc-modified-id=\"Data-structure-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data structure</a></span></li><li><span><a href=\"#get-features\" data-toc-modified-id=\"get-features-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>get features</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics\n",
    "\n",
    "## Structure cheat sheet\n",
    "\n",
    "1. func: train data lead (following order)\n",
    "    1. read the descriptive dataframe from the feature-pipeline\n",
    "    2. extract feature from the feature-objects which are labeled train-dataset from dataframe\n",
    "    3. create numpy feature array for the processing pipeline\n",
    "2. preprocessing\n",
    "    1. Transformation (any combination of the following)\n",
    "        + log-transform\n",
    "        + PCA\n",
    "        + others\n",
    "    2. Scaling (one of the following)\n",
    "        + StandardScaler\n",
    "        + MinMaxScaler\n",
    "3. Unsupervised Clustering\n",
    "    1. Estimate initial hyperparameter\n",
    "    2. Create grid over various hyperparameters\n",
    "    3. Train all and choose the best according to metric\n",
    "    \n",
    "    \n",
    "in all steps the cluster-recorder object (possibly dataframe-row) will record all the meta-information like hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data structure\n",
    "\n",
    "There are multiple degrees of freedom in the data:\n",
    "\n",
    "1. Signal to noise ratio (SNR)\n",
    "2. Machine type\n",
    "    1. pump\n",
    "    2. fan\n",
    "    3. valve (solenoid)\n",
    "    4. slider\n",
    "3. Machine ID\n",
    "    1. four different machine IDs\n",
    "    \n",
    "The pipeline will be applied to fixed SNR, fixed machine type and fixed ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get features\n",
    "\n",
    "Get the descriptive dataframe for the features.\n",
    "\n",
    "The descriptive dataframe contains all IDs of the pump. We will focus on ID '00' for now since the modeling phase is seperated per SNR, per machine, per ID anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class: \n",
    "+ uni\\_\\<model\\>\n",
    "attributes:\n",
    "+ default threshold\n",
    "+ roc_auc\n",
    "methods:\n",
    "+ fit\n",
    "+ predict\n",
    "+ predict_score\n",
    "+ eval_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T08:17:51.247069Z",
     "start_time": "2020-04-30T08:17:48.355226Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing numpy on engine(s)\n",
      "importing scipy on engine(s)\n",
      "importing pandas on engine(s)\n",
      "importing matplotlib.pyplot on engine(s)\n",
      "importing seaborn on engine(s)\n",
      "importing pickle on engine(s)\n",
      "importing tqdm from tqdm on engine(s)\n",
      "importing cycler from cycler on engine(s)\n",
      "importing librosa on engine(s)\n",
      "importing librosa.display on engine(s)\n",
      "importing os on engine(s)\n",
      "importing sys on engine(s)\n",
      "load feature_extractor_mother\n",
      "importing Enum from enum on engine(s)\n",
      "load feature_extractor_mel_spectra\n",
      "load feature_extractor_psd\n",
      "load feature_extractore_pre_nnFilterDenoise\n",
      "importing calinski_harabasz_score,davies_bouldin_score from sklearn.metrics on engine(s)\n",
      "importing train_test_split from sklearn.model_selection on engine(s)\n",
      "importing argrelextrema from scipy.signal on engine(s)\n",
      "importing EllipticEnvelope from sklearn.covariance on engine(s)\n",
      "importing IsolationForest from sklearn.ensemble on engine(s)\n",
      "importing OneClassSVM from sklearn.svm on engine(s)\n",
      "importing roc_auc_score from sklearn.metrics on engine(s)\n",
      "importing datetime from datetime on engine(s)\n",
      "importing time on engine(s)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#===============================================\n",
    "# Basic Imports\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "sns.set()\n",
    "\n",
    "BASE_FOLDER = '../../'\n",
    "%run -i ..\\..\\utility\\feature_extractor\\JupyterLoad_feature_extractor.py\n",
    "%run -i ..\\..\\utility\\modeling\\JupyterLoad_modeling.py\n",
    "\n",
    "\n",
    "#===============================================\n",
    "# Define the Model classes\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class uni_EllipticEnvelope(EllipticEnvelope):\n",
    "    def __init__(self, \n",
    "                 store_precision=True, \n",
    "                 assume_centered=False,\n",
    "                 support_fraction=None, \n",
    "                 contamination=0.1,\n",
    "                 random_state=None, \n",
    "                 def_threshold=0):\n",
    "\n",
    "        super().__init__(store_precision=store_precision,\n",
    "            assume_centered=assume_centered,\n",
    "            support_fraction=support_fraction,\n",
    "            contamination=contamination,\n",
    "            random_state=random_state)\n",
    "\n",
    "        self.def_threshold=def_threshold\n",
    "        self.roc_auc = None\n",
    "\n",
    "    #fit inherited    \n",
    "    # predict inherited\n",
    "\n",
    "    def predict_score(self, data):\n",
    "        return self.decision_function(data)\n",
    "\n",
    "    def eval_roc_auc(self, data_test, y_true):\n",
    "        return roc_auc_score(y_true, self.predict_score(data_test))\n",
    "\n",
    "class uni_IsolationForest(IsolationForest):\n",
    "    pass\n",
    "\n",
    "class uni_OneClassSVM(OneClassSVM):\n",
    "    pass\n",
    "\n",
    "\n",
    "#===============================================\n",
    "# define the Pipeline Class\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "class Pipe(object):\n",
    "    def __init__(self, preprocessing_steps=None, modeling_step=None):        \n",
    "        # instantiate evaluating parameters\n",
    "        self.roc_auc = None\n",
    "\n",
    "        # instantiate the preprocessing steps\n",
    "        self.preproc_steps = [step(**kwargs) for step, kwargs in preprocessing_steps]        \n",
    "\n",
    "        # create the predictive model             \n",
    "        self._mdl, self.model_args = modeling_step # model object\n",
    "        # model instance\n",
    "        self.model = self._mdl(**self.model_args)\n",
    "\n",
    "    def to_pickle(self, filepath=None):\n",
    "        self.update_filepath(filepath)\n",
    "\n",
    "        with open(self.filepath, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    def update_filepath(self, path=None):\n",
    "        if not path or (type(path)==dict):\n",
    "            if not path:\n",
    "                task = self.task\n",
    "            else:\n",
    "                task = path\n",
    "                self.filepath = '.\\\\pipes\\\\' + '_'.join([ task['feat_col'],\n",
    "                                    ''.join([str(i) for i in list(task['feat'].values())]),\n",
    "                                    task['SNR'],\n",
    "                                    task['machine'],\n",
    "                                    'ID'+task['ID'],\n",
    "                                    datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                                    ]) + '.pkl'\n",
    "        else:\n",
    "            self.filepath = path\n",
    "\n",
    "    def get_data(self, task):\n",
    "        time.sleep(.5)\n",
    "        self.df_train, data_train = load_data(train_set=True, **task)\n",
    "        self.df_test, data_test = load_data(train_set=False, **task)\n",
    "        self.ground_truth = self.df_test.abnormal.apply(lambda x : 1 if x==0 else -1)\n",
    "\n",
    "        # update filepath accordingly to task\n",
    "        self.update_filepath(task)\n",
    "\n",
    "        return data_train, data_test\n",
    "\n",
    "    def preprocess(self, data_train, data_test):\n",
    "        # run through all the preprocessing steps\n",
    "        for step in self.preproc_steps:\n",
    "            data_train =  step.fit_transform(data_train)\n",
    "            data_test = step.transform(data_test)\n",
    "\n",
    "        # return preprocessed data\n",
    "        return data_train, data_test\n",
    "\n",
    "    def fit_model(self, data_train):\n",
    "        # fit the model\n",
    "        self.model.fit(data_train)\n",
    "\n",
    "    def evaluate(self, data_test, ground_truth):\n",
    "        # calculate evaluation score\n",
    "\n",
    "        self.df_test['pred_scores'] = self.model.predict_score(data_test)\n",
    "        self.df_test['pred_labels'] = self.model.predict(data_test)\n",
    "        self.roc_auc = self.model.eval_roc_auc(data_test, ground_truth)\n",
    "\n",
    "    def run_pipe(self, task):\n",
    "        self.task = task\n",
    "        # get the data\n",
    "        print('...loading data')\n",
    "        data_train, data_test = self.get_data(task)\n",
    "        print('data loading completed\\n\\n...preprocessing data')\n",
    "\n",
    "        # preprocessing\n",
    "        data_train, data_test = self.preprocess(data_train, data_test)\n",
    "        print('data preprocessing finished\\n\\n...fitting the model')\n",
    "\n",
    "        # fitting the model\n",
    "        self.fit_model(data_train)\n",
    "        print('model fitted successfully\\n\\n...evaluating model')\n",
    "\n",
    "        # evaluating over ground truth\n",
    "        self.evaluate(data_test, self.ground_truth)\n",
    "        print('evaluation successfull, roc_auc:', self.roc_auc)\n",
    "\n",
    "        # saving to pickle\n",
    "        self.to_pickle()\n",
    "        print('pipe saved to pickle')\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T08:17:51.308074Z",
     "start_time": "2020-04-30T08:17:51.256069Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing StandardScaler from sklearn.preprocessing on engine(s)\n",
      "importing PCA,FastICA from sklearn.decomposition on engine(s)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "\n",
    "preprocessing = [\n",
    "    (FastICA, {'n_components':40, 'algorithm':'parallel'}),\n",
    "    (StandardScaler, {})\n",
    "]\n",
    "\n",
    "modeling = (uni_EllipticEnvelope, {'random_state':42})\n",
    "\n",
    "IDs = [\n",
    "    '00',\n",
    "    '02',\n",
    "    '04',\n",
    "    '06'\n",
    "      ]\n",
    "\n",
    "machines = [\n",
    "    'pump',\n",
    "    'slider',\n",
    "    'fan',\n",
    "    'valve'\n",
    "]\n",
    "\n",
    "paths = [BASE_FOLDER\n",
    "         +'dataset/MEL_to_Pandas/data_6dB_{}/FEpandas_MELv1_nm80_ch0.pkl'.format(machine) \n",
    "         for machine in machines]\n",
    "\n",
    "tasks = [{\n",
    "    'path_descr':path, \n",
    "    'feat':{'function':'frame', 'frames':5}, \n",
    "    'feat_col':'MELv1_nm80_ch0', \n",
    "    'SNR':'6dB', \n",
    "    'machine':machine, \n",
    "    'ID':ID,\n",
    "    'BASE_FOLDER':BASE_FOLDER\n",
    "    } for ID in IDs for machine, path in zip(machines, paths)]\n",
    "\n",
    "pipes = [Pipe(preprocessing, modeling) for i in range(len(tasks))]\n",
    "\n",
    "# make a function to call on each worker\n",
    "def build_run_pipe(task, preprocessing, modeling):\n",
    "    pipe = Pipe(preprocessing, modeling)\n",
    "    return pipe.run_pipe(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T08:17:51.373067Z",
     "start_time": "2020-04-30T08:17:48.285Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example settings\n",
    "n_samples = 200\n",
    "outliers_fraction = 0.25\n",
    "clusters_separation = [0, 1, 2]\n",
    "\n",
    "# define two outlier detection tools to be compared\n",
    "classifiers = {\n",
    "    \"One-Class SVM\": svm.OneClassSVM(nu=0.95 * outliers_fraction + 0.05,\n",
    "                                     kernel=\"rbf\", gamma=0.1),\n",
    "    \"Robust covariance\": EllipticEnvelope(contamination=outliers_fraction),\n",
    "    \"Isolation Forest\": IsolationForest(max_samples=n_samples,\n",
    "                                        contamination=outliers_fraction,\n",
    "                                        random_state=rng)}\n",
    "\n",
    "for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "    # fit the data and tag outliers\n",
    "    clf.fit(X)\n",
    "    scores_pred = clf.decision_function(X)\n",
    "    threshold = stats.scoreatpercentile(scores_pred,\n",
    "                                        100 * outliers_fraction)\n",
    "    y_pred = clf.predict(X)\n",
    "    n_errors = (y_pred != ground_truth).sum()\n",
    "    # plot the levels lines and the points\n",
    "    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    subplot = plt.subplot(1, 3, i + 1)\n",
    "    subplot.contourf(xx, yy, Z, levels=np.linspace(Z.min(), threshold, 7),\n",
    "                        cmap=plt.cm.Blues_r)\n",
    "    a = subplot.contour(xx, yy, Z, levels=[threshold],\n",
    "                        linewidths=2, colors='red')\n",
    "    subplot.contourf(xx, yy, Z, levels=[threshold, Z.max()],\n",
    "                        colors='orange')\n",
    "    b = subplot.scatter(X[:-n_outliers, 0], X[:-n_outliers, 1], c='white')\n",
    "    c = subplot.scatter(X[-n_outliers:, 0], X[-n_outliers:, 1], c='black')\n",
    "    subplot.axis('tight')\n",
    "    subplot.legend(\n",
    "        [a.collections[0], b, c],\n",
    "        ['learned decision function', 'true inliers', 'true outliers'],\n",
    "        prop=matplotlib.font_manager.FontProperties(size=11),\n",
    "        loc='lower right')\n",
    "    subplot.set_title(\"%d. %s (errors: %d)\" % (i + 1, clf_name, n_errors))\n",
    "    subplot.set_xlim((-7, 7))\n",
    "    subplot.set_ylim((-7, 7))\n",
    "plt.subplots_adjust(0.04, 0.1, 0.96, 0.92, 0.1, 0.26)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3.7 (mimii_base_TF2_GPU)",
   "language": "python",
   "name": "mimii-tf2-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
